[TOC]

# LangChain ä»‹ç»

æ•´ä½“æµç¨‹ï¼š

<u>æ–‡æ¡£é€šè¿‡ pdf è¯»å– + åˆ†è¯ + å‘é‡åŒ– ä¿å­˜åˆ°æ•°æ®åº“ä¸­</u>

<u>æ ¹æ®æé—®æ£€ç´¢æ•°æ®åº“ï¼Œæ‰¾å‡ºç›¸å…³ä¸Šä¸‹æ–‡æ®µè½</u>

<u>LangChain å°† ä¸Šä¸‹æ–‡ + æé—® æ•´åˆåˆ°ä¸€èµ·ï¼Œè¾“å…¥ç»™ ada/davinci/chatgpt ç­‰è¯­è¨€æ¨¡å‹</u>

<u>è¿”å›å­—ç¬¦ä¸²ï¼Œè¿›è¡Œè¾“å‡º</u>

å‚è€ƒé¡¹ç›®ï¼š

LangChain - æ‰“é€ è‡ªå·±çš„GPTï¼ˆäºŒï¼‰simple-chatpdf

https://github.com/HappyGO2023/simple-chatpdf

https://zhuanlan.zhihu.com/p/620422560

åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM åº”ç”¨å®ç°

https://github.com/imClumsyPanda/langchain-ChatGLM

## 1. å‡†å¤‡ç¯å¢ƒ

### 1.1 æ–°å»ºç¯å¢ƒ

åˆ›å»ºç¯å¢ƒ

```shell
conda activate -n chat python=3.9
```

conda æ¢æºï¼ˆå¯ä»¥è·³è¿‡ï¼‰

```
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes

conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/
conda config --set show_channel_urls yes

conda clean -i
```

### 1.2 å®‰è£…ä¾èµ–

conda å¤ªæ…¢äº†ï¼Œè€ƒè™‘ç”¨ pip å®‰è£…

é¦–å…ˆå…ˆè£… torchï¼Œç›´æ¥ä¸‹è½½æœ¬åœ°æ–‡ä»¶ï¼ˆéœ€è¦æŒ‰ç…§è‡ªå·±çš„ cuda ç‰ˆæœ¬è¿›è¡Œé€‰æ‹©ï¼‰

https://download.pytorch.org/whl/cu117/torch-2.0.0%2Bcu117-cp39-cp39-win_amd64.whl

```shell
# åœ¨çº¿å®‰è£…
pip install torch --index-url https://download.pytorch.org/whl/cu117
# æœ¬åœ°å®‰è£…ï¼ˆæ¨èï¼‰
pip install torch-2.0.0+cu117-cp39-cp39-win_amd64.whl
```

ç„¶åè£…ä¾èµ–ï¼Œä¸»è¦æ˜¯ langchain åº“å’Œ openai åº“

å¯ä»¥å‚è€ƒ https://github.com/HappyGO2023/simple-chatpdf çš„ requirements.txt

```shell
# ä¸€é”®å®‰è£…
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
# langchainä¾èµ–
langchain
PyPDF2
chromadb
tiktoken
# openaiä¾èµ–
openai
# chatglmä¾èµ–
protobuf
transformers==4.27.1
cpm_kernels
torch>=1.10
sentencepiece
accelerate
```

å¦‚æœåç»­è°ƒç”¨ OpenAI çš„æ—¶å€™æŠ¥é”™ï¼Œåˆ™éœ€è¦é™çº§ urllib3

```shell
pip install urllib3==1.25.11
```

## 2. å¼€é€š OpenAI API

### 2.1 è´¦å·å…è´¹é¢åº¦

LangChain æ•´åˆäº†å¤šç§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ openai apiã€æœ¬åœ°æ¨¡å‹ç­‰

<u>ä½œä¸ºå®éªŒï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨ OpenAI çš„æ¥å£ï¼Œç„¶åå†æ¢æˆæœ¬åœ°çš„æ¨¡å‹</u>

<u>æ¯ä¸ª OpenAI è´¦å·é‡Œä¼šæœ‰å…è´¹çš„é¢åº¦ï¼Œéœ€è¦æ³¨æ„æ£€æŸ¥æœ‰æ²¡æœ‰è¿‡æœŸ</u>

\$5 å…è´¹é¢åº¦ï¼Œè°ƒç”¨ gpt-3.5-turbo \$0.002 / 1K tokensï¼Œå¯ä»¥ä½¿ç”¨å¾ˆä¹…

ä¸‹é¢çš„æç¤ºæ˜¯æ²¡é’±çš„æŠ¥é”™ï¼š

```
Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..
```

### 2.2 ä¸åŒæ¨¡å‹çš„ä»·æ ¼

https://platform.openai.com/account/usage æŸ¥è¯¢è´¦å·çš„ç”¨é‡

https://platform.openai.com/docs/models/gpt-3-5 æŸ¥çœ‹ä¸åŒçš„æ¨¡å‹

https://openai.com/pricing#language-models æŸ¥çœ‹ä¸åŒçš„ä»·æ ¼

ä¸»è¦æœ‰ä¸‰ç§æ¨¡å‹ï¼šGPT-4ï¼ŒGPT-3.5 (Chat)ï¼ŒGPT-3 (InstructGPT)

<u>è¯­è¨€æ¨¡å‹å¯ä»¥é€‰ gpt-3.5-turbo (ChatGPT) æˆ–è€… ada</u>

<u>Embedding æ¨¡å‹ç›®å‰åªæœ‰ ada å¼€æ”¾ï¼Œä¸éœ€è¦é€‰æ‹©</u>

| Model         | Price / 1K tokens |
| ------------- | ----------------- |
| gpt-4         | $0.03             |
| gpt-4-32k     | $0.06             |
| gpt-3.5-turbo | $0.002            |
| davinci       | $0.02             |
| curie         | $0.002            |
| babbage       | $0.0005           |
| ada           | $0.0004           |

https://codechina.org/2023/02/openai-gpt-api-summarize/ GPT-3 å„æ¨¡å‹åŒºåˆ«

LangChain é‡Œé¢çš„é»˜è®¤æ¨¡å‹ï¼š

**OpenAI**ç±»é»˜è®¤å¯¹åº” â€œtext-davinci-003â€ ç‰ˆæœ¬ï¼š

```python3
OpenAI(temperature=0)
```

**OpenAIChat**ç±»é»˜è®¤æ˜¯ "gpt-3.5-turbo"ç‰ˆæœ¬ï¼š

```python3
OpenAIChat(temperature=0)
```



## 3. è¯•éªŒ OpenAI æ¥å£ (æ™®é€š LLM)

å» Anaconda è£…ä¸€ä¸ª JupyterLabï¼Œå¼€ä¸€ä¸ªç¬”è®°æœ¬

### 3.1 è®¾ç½®ç¯å¢ƒä¿¡æ¯

ç„¶åå¡«å…¥ç¯å¢ƒä¿¡æ¯ï¼Œè®¿é—® OpenAI éœ€è¦æŒ‚ä»£ç†

https://github.com/zhayujie/chatgpt-on-wechat/issues/351

æˆ‘ç”¨ SSR å¼€å¯ â€œå…è®¸æ¥è‡ªå±€åŸŸç½‘çš„è¿æ¥â€ï¼Œå°†ç«¯å£è®¾æˆ 1080

ç½‘ç»œæµå‘ï¼šPython - 1080 ç«¯å£ - SSR - OpenAI

å¦‚æœæ˜¯ä¸åŒçš„ä»£ç†è½¯ä»¶ï¼Œå¯èƒ½éœ€è¦é…ç½®ä¸åŒç«¯å£

```python
import os
# å¡«å…¥è‡ªå·±çš„ OpenAI Key
os.environ["OPENAI_API_KEY"] = "Key"
# è®¾ç½®ä»£ç†ç«¯å£
os.environ["HTTP_PROXY"] = "127.0.0.1:1080"
os.environ["HTTPS_PROXY"] = "127.0.0.1:1080"
```

### 3.2 ä½¿ç”¨æ™®é€š LLMï¼ˆadaï¼‰é—®ç­”

è®¾ç½®è¯­è¨€æ¨¡å‹

```python
from langchain.llms import OpenAI
llm = OpenAI(model_name="text-ada-001", temperature=0.9)
```

ç„¶åå°±å¯ä»¥å¼€å§‹é—®ç­”äº†ï¼Œåªéœ€è¦è¾“å…¥ä½ çš„æŒ‡ä»¤æˆ–è€…é—®é¢˜

```python
text = "Please introduce yourself."
print(llm(text))
```

å¥‡å¥‡æ€ªæ€ªçš„å›ç­”

```
I am a 28-year-old singleartist who is looking for a relationship that is serious and interested in 2-3 years of experience in the artist base.
```

è®¾ç½®ä¸€ä¸ª prompt æ¨¡æ¿ï¼Œå¯ä»¥å‘æ¨¡æ¿å¡«å…¥å‚æ•°ï¼Œä¸ç”¨æ¯æ¬¡å†™ä¸€ä¸ªå®Œæ•´çš„ prompt

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["chara"],
    template="Introduce yourself as {chara}",
)
```

```python
prompt.format(chara="a musician")

'Introduce yourself as a musician in one sentence.'
```

åˆ›å»ºä¸€ä¸ª chainï¼Œå°† llm å’Œ prompt ç»“åˆèµ·æ¥ï¼Œå®ç°ä¸€é”®è°ƒç”¨

```python
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)

result = chain.run("the Dragonborn of The Elder Scrolls")
print(result)
```

ada è¿”å›çš„ç»“æœï¼Œæœ‰ç‚¹ä¹±

```python
I am the Dragonborn, the invoking and battle-ready! I am the â€”

Frosty, the Tauren player character who began the trend ofistic skin blue eyes and The Elder ScrollsSSSS

I am the Dragonborn, the invoking and battle-ready! I am the

Tauren, the infant god- alerted by the call of the rising sun! I am

the Tauren, the infant god- alerted by the call of the rising sun!
```



## 4. è¯•éªŒ ChatModels æ¥å£

LLM æ¥å£æ˜¯è¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œè¾“å‡ºä¸€æ®µæ–‡æœ¬

Chat models åˆ™æ˜¯ LLM çš„ä¸€ä¸ªå˜ç§ï¼Œæ¥å£å¤æ‚ä¸€ç‚¹ï¼Œä»ç®€å•æ–‡æœ¬å˜æˆäº†èŠå¤©ä¿¡æ¯

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
```

é€šè¿‡ HumanMessage å’Œæ¨¡å‹è¿›è¡Œäº¤äº’

```python
messages = [HumanMessage(content="è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±.")]

result = chat(messages)
```

```python
content='æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œè¢«ç§°ä¸ºOpenAIçš„GPT-3ã€‚æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œç”Ÿæˆæ–‡ç« ã€å¯¹è¯å’Œå…¶ä»–æ–‡æœ¬å½¢å¼çš„å†…å®¹ã€‚æˆ‘å¯ä»¥å­¦ä¹ å’Œç†è§£ä¸åŒçš„è¯­è¨€å’Œä¸»é¢˜ï¼Œå¹¶å°½å¯èƒ½åœ°å›ç­”é—®é¢˜å’Œæä¾›æœ‰ç”¨çš„ä¿¡æ¯ã€‚' additional_kwargs={}
```

é€šè¿‡ SystemMessage å¯¹æ¨¡å‹æä¾›å¯¹è¯çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå†ç”¨ HumanMessage åšå…·ä½“é—®ç­”

```python
messages = [
    SystemMessage(content="You are the Dragonborn in The Elder Scrolls."),
    HumanMessage(content="è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±.")
]

result = chat(messages)
```

```python
content='æˆ‘æ˜¯é¾™è£”ï¼Œåˆç§°ä¸ºé¾™é™ä¸´è€…ï¼Œæ˜¯ã€Šä¸Šå¤å·è½´ã€‹ç³»åˆ—æ¸¸æˆä¸­çš„ä¸»è§’ã€‚æˆ‘æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æˆ˜å£«ã€æ³•å¸ˆå’Œç›—è´¼ï¼Œå¯ä»¥ä½¿ç”¨é¾™è¯­æ³•æœ¯å’Œæ­¦å™¨æ¥æ‰“è´¥æ•Œäººã€‚æˆ‘çš„ä»»åŠ¡æ˜¯æŠµå¾¡é¾™çš„å…¥ä¾µï¼Œå¹¶æ‹¯æ•‘å¤©é™…çœçš„äººæ°‘ã€‚æˆ‘è¿˜å¯ä»¥åŠ å…¥å„ç§ç»„ç»‡ï¼Œå¦‚é»‘æš—å…„å¼Ÿä¼šã€ç›—è´¼å…¬ä¼šå’Œæˆ˜å£«å…¬ä¼šï¼Œå¹¶å®Œæˆè®¸å¤šä»»åŠ¡å’Œå‰¯æœ¬ã€‚æ€»ä¹‹ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè‹±å‹‡çš„å†’é™©å®¶ï¼Œè‡´åŠ›äºä¿æŠ¤å¤©é™…çœå’Œå±…æ°‘çš„å®‰å…¨ã€‚' additional_kwargs={}
```

SystemMessage æ˜¯ç³»ç»Ÿå‘Šè¯‰ gpt çš„ä¿¡æ¯ï¼Œå¯ä»¥è¦æ±‚ gpt ä½œä¸ºä¸€ä¸ªæ€ä¹ˆæ ·çš„èº«ä»½å»äº¤äº’ï¼ˆä½ æ˜¯ä¸€ä¸ªå¯¹äººç±»å‹å¥½çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼‰ï¼Œå¹¶ä¸”æä¾›ä¸€äº›è¦æ±‚ï¼ˆä½ çš„å›ç­”éœ€è¦å‹å¥½ã€ç†æ€§ã€å®Œå–„ï¼Œæ‹’ç»ä¸é“å¾·çš„æé—®ï¼‰

HumanMessage æ˜¯äººç±»ç”¨æˆ·å¯¹ gpt çš„é—®ç­”å†…å®¹



èŠå¤©ä¿¡æ¯ç›¸åº”çš„ä¹Ÿä¼šæœ‰ Template å’Œ Chain

å¯¹åº”çš„ Agent å’Œ Memory åˆ™ä¼šæ›´åŠ å¤æ‚ä¸€ç‚¹



## 5. LangChain çš„åŸºæœ¬æ¦‚å¿µ

### 5.1 Models

æ¨¡å‹å¯ä»¥æ˜¯ OpenAI æ¥å£ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹

LLMsï¼šæ™®é€šè¯­è¨€æ¨¡å‹ï¼Œè¾“å…¥ä¸€æ®µè¯è¾“å‡ºä¸€æ®µè¯

Chat Modelsï¼šå¯¹è¯æ¨¡å‹ï¼Œè¾“å…¥è¾“å‡ºçš„æ ¼å¼åŒ…æ‹¬AIä¿¡æ¯ã€ç³»ç»Ÿä¿¡æ¯ã€ç”¨æˆ·ä¿¡æ¯ç­‰ï¼Œæ”¯æŒæµå¼å›ç­”ï¼ˆä¸€ä¸ªä¸€ä¸ªå­—æ‰“å‡ºæ¥ï¼‰

Text Embedding Modelsï¼šç”¨äºæ–‡æœ¬å‘é‡åŒ–ï¼Œè¾“å…¥æ–‡æœ¬è¾“å‡ºä¸€ç»„æµ®ç‚¹æ•°

### 5.2 Prompt/PromptTemplate

æç¤ºå’Œæç¤ºæ¨¡æ¿ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥

### 5.3 Chain

æµç¨‹é“¾ï¼Œç®€å•çš„é“¾å°±ä¸‰æ­¥ï¼šä»æ¨¡æ¿åˆ›å»º promptï¼Œå°† prompt è¾“å…¥åˆ°æ¨¡å‹ï¼Œå¾—åˆ°è¾“å‡ºã€‚å…¶ä¸­è¾“å…¥æ¨¡å‹å’Œè·å–è¾“å‡ºæ˜¯ä¸€èµ·çš„

**chain_type**

stuffï¼šç›´æ¥å°†æ‰€æœ‰ doc ä¸¢ç»™ llmï¼Œå¯èƒ½ä¼šè¶… token

map_reduceï¼šæ¯ä¸ª doc è¿›è¡Œæ€»ç»“ï¼Œå†åšæ•´ä½“æ€»ç»“

refineï¼šæ€»ç»“1+doc2=æ€»ç»“2ï¼Œæ€»ç»“2+doc3=æ€»ç»“3ï¼Œ......

map_rerankï¼šå¯¹æ¯ä¸ª doc è®¡ç®—åŒ¹é…åº¦ï¼Œé€‰æ‹©æœ€é«˜åˆ†æ•°çš„ doc ç»™ llm åšå›ç­”

### 5.4 Agent

https://zhuanlan.zhihu.com/p/619344042 è¯¦è§£

LLMä½œä¸ºé©¾é©¶å‘˜ï¼Œæ ¹æ®ç”¨æˆ·çš„è¾“å…¥åŠ¨æ€è°ƒç”¨ Chain/Tool

æ ¸å¿ƒæ¦‚å¿µï¼š

- Toolï¼šä¾‹å¦‚ google æœç´¢ã€æ•°æ®åº“æ£€ç´¢ã€Python ç­‰ç­‰ï¼Œå¯ä¾›è°ƒç”¨ã€‚ä¸€èˆ¬è¾“å…¥è¾“å‡ºéƒ½æ˜¯å­—ç¬¦ä¸²ã€‚æ¯ä¸ª tool éƒ½æœ‰ä¸€æ®µè¯­è¨€æè¿°ï¼Œç›¸å½“äºè¯´æ˜ä¹¦
- LLMï¼šè¯­è¨€æ¨¡å‹
- Agentï¼šéœ€è¦ä½¿ç”¨çš„ä»£ç†ã€‚`zero-shot-react-description`ã€`react-docstore` ç­‰å¤šç§é¢„è®¾ç±»å‹

`zero-shot-react-description`ï¼šæ ¹æ®æ¯ä¸ª tools çš„æè¿°ï¼Œé€‰æ‹©å‡ºéœ€è¦çš„ tool

```python
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")
```

```
> Entering new AgentExecutor chain...
 I need to find the temperature first, then use the calculator to raise it to the .023 power.
Action: Search
Action Input: "High temperature in SF yesterday"
Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 Â°F (at 1:56 pm) Minimum temperature yesterday: 49 Â°F (at 1:56 am) Average temperature ...
Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.
Action: Calculator
Action Input: 57^.023
Observation: Answer: 1.0974509573251117

Thought: I now know the final answer
Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.

> Finished chain.
```

https://liaokong.gitbook.io/llm-kai-fa-jiao-cheng/

![](https://pic4.zhimg.com/v2-1274c950395603397847feb23e2c3a0b_r.jpg)

### 5.5 Memory

å‘ Chain å’Œ Agent æ·»åŠ çŠ¶æ€ï¼Œä¾‹å¦‚çŸ­æœŸè®°å¿†æˆ–é•¿æœŸè®°å¿†

```python
from langchain.memory import ChatMessageHistory
from langchain.chat_models import ChatOpenAI

chat = ChatOpenAI(temperature=0)

# åˆå§‹åŒ– MessageHistory å¯¹è±¡
history = ChatMessageHistory()

# ç»™ MessageHistory å¯¹è±¡æ·»åŠ å¯¹è¯å†…å®¹
history.add_ai_message("ä½ å¥½ï¼")
history.add_user_message("ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ")

# æ‰§è¡Œå¯¹è¯
ai_response = chat(history.messages)
print(ai_response)
```



## 6. simple-chatpdf

é¡¹ç›®åœ°å€

https://github.com/HappyGO2023/simple-chatpdf

é¦–å…ˆå‚è€ƒ embedding.py è¿›è¡ŒçŸ¥è¯†åº“å­˜å‚¨ï¼Œç„¶åå‚è€ƒ qa.py è¿›è¡ŒçŸ¥è¯†åº“é—®ç­”

jupyter notebook å®ç°



æ•´ä½“æµç¨‹

1. æŠŠä½ çš„å†…å®¹æ‹†æˆä¸€å—å—çš„å°æ–‡ä»¶å—ã€å¯¹å—è¿›è¡Œäº†Embeddingåæ”¾å…¥å‘é‡åº“ç´¢å¼• ï¼ˆä¸ºåé¢æä¾›è¯­ä¹‰æœç´¢åšå‡†å¤‡ï¼‰ã€‚

2. æœç´¢çš„æ—¶å€™æŠŠQueryè¿›è¡ŒEmbeddingåé€šè¿‡è¯­ä¹‰æ£€ç´¢æ‰¾åˆ°æœ€ç›¸ä¼¼çš„Kä¸ªDocsã€‚

3. æŠŠç›¸å…³çš„Docsç»„è£…æˆPromptçš„Contextï¼ŒåŸºäºç›¸å…³å†…å®¹è¿›è¡ŒQAï¼Œè®©GPTè¿›è¡ŒIn Context Learningï¼Œç”¨äººè¯å›ç­”é—®é¢˜ã€‚

### 6.1 è½½å…¥ PDF

ä½¿ç”¨ PyPDF2.PdfReader å°† pdf åŠ è½½è¿›æ¥

```python
import PyPDF2

pdf_path = 'KOSï¼š2023ä¸­å›½å¸‚åœºæ‹›è˜è¶‹åŠ¿.pdf'
pdf_file = open(pdf_path, 'rb')
pdf_reader = PyPDF2.PdfReader(pdf_file)

pdf_content = ''
for num in range(len(pdf_reader.pages)):
    page = pdf_reader.pages[num]
    pdf_content += page.extract_text()
```

pdf_reader.pages[x] å°±æ˜¯æ¯ä¸€é¡µçš„å†…å®¹ï¼ŒåŒ…æ‹¬äº†å¾ˆå¤šæ ¼å¼ä¹‹ç±»çš„ä¿¡æ¯

pdf_reader.pages[x].extract_text() å°†æ¯ä¸€é¡µçš„æ–‡å­—æå–å‡ºæ¥ï¼Œå­˜å…¥ pdf_cotent å­—ç¬¦ä¸²

ä¸€ä¸ª 89 é¡µçš„ PDF å¯ä»¥æœ‰ 34400 å­—

### 6.2 æ–‡æ¡£æ¸…æ´—

å…¨æ–‡æ¡£å­—ç¬¦ä¸² >> åˆ†å¥ >> å¤šå¥åˆå¹¶æˆæ®µ

pdf_content å°†æ‰€æœ‰å†…å®¹ä¿å­˜æˆä¸€ä¸ªå­—ç¬¦ä¸²

é¦–å…ˆéœ€è¦å°†æ¢è¡Œæ´—æ‰

ç„¶åæŒ‰ç…§å¥å­ç»“æŸç¬¦å·ï¼ˆï¼›ã€‚ï¼ï¼Ÿç­‰ï¼‰ï¼Œæ‹†åˆ†æˆä¸€å¥å¥è¯

```python
pdf_content = pdf_content.replace('\n', '') 
pdf_content = pdf_content.replace('\n\n', '') 
pdf_content = re.sub(r'\s+', ' ', pdf_content)

pdf_sentences_mark = re.split('(ï¼›|ã€‚|ï¼|\!|\.|ï¼Ÿ|\?)', pdf_content) 

pdf_sentences = []
for i in range(int(len(pdf_sentences_mark)/2)):
    sent = pdf_sentences_mark[2*i] + pdf_sentences_mark[2*i+1]
    pdf_sentences.append(sent)
if len(pdf_sentences_mark) % 2 == 1:
    pdf_sentences.append(pdf_sentences_mark[len(pdf_sentences_mark)-1])
```

æ¥ä¸‹æ¥å°†å¤šä¸ªå¥å­æ‹¼æˆä¸€æ®µï¼ŒæŒ‰ç…§æœ€å¤§é•¿åº¦ 300 ä¸ºä¸€æ®µ

æœ€åå¾—åˆ°ä¸€ä¸ª paragraphsï¼Œlist ç±»å‹ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€æ®µå­—ç¬¦ä¸²ï¼Œlen < 300

è¿™æ ·æˆ‘ä»¬å°±æŠŠå®Œæ•´çš„ PDF æ‹†åˆ†æˆäº†ä¸€æ®µæ®µå°æ–‡æœ¬

ä¹Ÿå¯ä»¥ç”¨ langchain.text_splitter.CharacterTextSplitter æ¥å°è¯•

```python
paragraphs = []
max_len = 300
current_len = 0
current_para = ""

for sent in pdf_sentences:
    sent_len = len(sent)
    if current_len + sent_len <= max_len:
        current_para += sent
        current_len += sent_len
    else:
        paragraphs.append(current_para.strip())
        current_para = sent
        current_len = sent_len

paragraphs.append(current_para.strip())
```

### 6.3 æ–‡æ¡£ä¿å­˜

æ–‡å­—éœ€è¦ä¿å­˜æˆ langchain.docstore.document.Document ç±»å‹

åˆ†å¥½çš„æ¯ä¸€æ®µæ˜¯ä¸€ä¸ªå° Document

åŒ…å« page_content æ–‡æœ¬å†…å®¹ï¼Œmetadata æ˜¯è‡ªå·±å¡«å†™çš„ä¿¡æ¯å­—æ®µï¼Œç”¨äºæ•°æ®åº“æ£€ç´¢

```python
Document(page_content='xxx', metadata={'source':'xxx, ...})
```

æŠŠæ¯ä¸ªå° Document æ”¾è¿›ä¸€ä¸ª documents list é‡Œ

æ•´ä¸ª documents[ ] å°±æ˜¯å®Œæ•´çš„ pdf

```python
from langchain.docstore.document import Document

documents = []
metadata = {"source": pdf_path}
for para in paragraphs:
    new_doc = Document(page_content=para, metadata=metadata)
    documents.append(new_doc)
```

### 6.4 æ–‡æ¡£ Embedding æŒä¹…åŒ–

è¿™äº› Documents éœ€è¦å˜æˆæ–‡ä»¶ä¿å­˜èµ·æ¥

LangChain æä¾›äº†ä¸€ä¸ªå‘é‡æ•°æ®åº“ Chroma

éœ€è¦å®‰è£… `pip install tiktoken`

å°† documents ä¼ å…¥ Chromaï¼Œç”¨ OpenAI æ¥å£åšæˆ Embeddingsï¼Œç„¶åæŒä¹…åŒ–åˆ° db_openai ç›®å½•

```python
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

embedding = OpenAIEmbeddings()
persist_directory = 'db_openai'

vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory)

vectordb.persist()
```

### 6.5 æœ¬åœ° Embedding 

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•´ä¸ªé¡¹ç›®

https://huggingface.co/shibing624/text2vec-base-chinese

å¯èƒ½éœ€è¦å®‰è£… text2vec åº“ï¼Œä¸çŸ¥é“æ˜¯å¦å¿…è¦

```shell
pip install text2vec
```

å°† OpenAI Embedding æ›¿æ¢æˆæœ¬åœ° Embedding

æ³¨æ„æŒä¹…åŒ–ç›®å½•ï¼Œä¸åŒ Embedding æ¨¡å‹çš„ç›®å½•éœ€è¦åŒºåˆ†ï¼Œå¦åˆ™ä¼šåœ¨å†™å…¥æ—¶æŠ¥é”™

ï¼ˆå¯ä»¥è¯•ä¸€ä¸‹æ”¹æˆ persist_directory = 'db_openai'ï¼Œçœ‹çœ‹å‘ç”Ÿä»€ä¹ˆé”™è¯¯ï¼‰

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

embedding = HuggingFaceEmbeddings(model_name=r'D:\Projects\text2vec-base-chinese')
persist_directory = 'db_huggingface'

vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory)

vectordb.persist()
```

### 6.6 æ•°æ®åº“è¯»å–

ç”¨ Chroma.as_retriever åˆ›å»ºä¸€ä¸ª retrieverï¼Œä½œä¸ºæ•°æ®åº“çš„æ£€ç´¢å™¨

```python
vectordb = Chroma(persist_directory='db', embedding_function=embedding)
retriever = vectordb.as_retriever(search_kwargs={"k": 3})
```

æ ¹æ® queryï¼Œç”¨ retriever æŸ¥æ‰¾å‡ºæœ€ç›¸å…³çš„ k ä¸ª Documentï¼ˆæ®µè½ï¼‰

åˆ›å»ºä¸€ä¸ª prompt_templateï¼Œéœ€è¦å¡«å…¥çš„å‚æ•°ä¸º context å’Œ query

```python
from langchain.prompts import PromptTemplate

prompt_template="""è¯·æ³¨æ„ï¼šè¯·è°¨æ…è¯„ä¼°Queryä¸æç¤ºçš„Contextä¿¡æ¯çš„ç›¸å…³æ€§ï¼Œåªæ ¹æ®æœ¬æ®µè¾“å…¥æ–‡å­—ä¿¡æ¯çš„å†…å®¹è¿›è¡Œå›ç­”ï¼Œå¦‚æœQueryä¸æä¾›çš„ææ–™æ— å…³ï¼Œè¯·å›ç­”"æˆ‘ä¸çŸ¥é“"ï¼Œå¦å¤–ä¹Ÿä¸è¦å›ç­”æ— å…³ç­”æ¡ˆï¼š
    Context: {context}
    Query: {query}
    Answer:"""

prompt = PromptTemplate(
    input_variables=["context", "query"],
    template=prompt_template
)

query = '2022å¹´å›½å†…æ–°èƒ½æºè½¦çš„æ¸—é€ç‡æ˜¯å¤šå°‘?'
docs = retriever.get_relevant_documents(query)
```

LangChain ä¸­è‡ªå¸¦ load_qa_chainï¼Œå¯ä»¥æ•´åˆ ChatOpenAI å’Œ Promptï¼ŒåŸºäº docs è¿›è¡Œé—®ç­”

```python
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI

chain = load_qa_chain(ChatOpenAI(temperature=0), chain_type="stuff", prompt=prompt)
```

è¿è¡Œ chain æ—¶å¡«å…¥å‚æ•°ï¼Œcontext = æŸ¥æ‰¾å‡ºæ¥çš„ docsï¼Œquery = ä½ çš„æé—®

```python
result = chain({"input_documents": docs, "query": query}, return_only_outputs=True)
print(result)
```



## 7. LangChain + ChatGLM

### 7.1 LangChain å¼•å…¥ ChatGLM ç±»

å°½ç®¡ LangChain æ”¯æŒäº† OpenAIã€LLaMAã€GPT4ALLã€Hugging Face ç­‰å¤šç§æ¨¡å‹ï¼Œä½†æ˜¯æ²¡æœ‰é¢„è®¾çš„ ChatGLM ç±»ã€‚å› æ­¤éœ€è¦è‡ªå·±åˆ›å»ºä¸€ä¸ªç±»

https://github.com/imClumsyPanda/langchain-ChatGLM

ç±»çš„å®ç°å‚è€ƒ models/chatllm.py

ä¸»è¦åŸºäº LangChain çš„ LLM åŸºç±»ï¼Œåˆ›å»ºäº† ChatGLM ç±»

å®šä¹‰ _call å’Œ load_model æ–¹æ³•

ï¼ˆå…·ä½“çš„æ–¹æ³•å‚è€ƒ 8.8 Custom LLMï¼‰

```python
from langchain.llms.base import LLM
from typing import List
from transformers import AutoTokenizer, AutoModel, AutoConfig
import torch


def torch_gc():
    # with torch.cuda.device(DEVICE):
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    

class ChatGLM(LLM):
    max_token: int = 10000
    temperature: float = 0.01
    top_p = 0.9
    # history = []
    tokenizer: object = None
    model: object = None
    history_len: int = 10

    def __init__(self):
        super().__init__()

    @property
    def _llm_type(self) -> str:
        return "ChatGLM"

    def _call(self,
              prompt: str,
              history: List[List[str]] = []):  # -> Tuple[str, List[List[str]]]:
        response, _ = self.model.chat(
            self.tokenizer,
            prompt,
            history=history[-self.history_len:] if self.history_len > 0 else [],
            max_length=self.max_token,
            temperature=self.temperature,
        )
        torch_gc()
        history += [[prompt, response]]
        yield response, history
        torch_gc()

    def load_model(self,
                   model_name_or_path: str = 'chatglm-6b-int4',
                   **kwargs):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            trust_remote_code=True
        )
        model_config = AutoConfig.from_pretrained(
            model_name_or_path, 
            trust_remote_code=True)
        self.model = AutoModel.from_pretrained(
            model_name_or_path, 
            config=model_config, 
            trust_remote_code=True,
            **kwargs).half().cuda()
        self.model = self.model.eval()
```

### 7.2 ç®€å•ä½¿ç”¨

å¼•å…¥ ChatGLM å’Œå¼•å…¥ OpenAI ç±»ä¼¼ï¼Œä½†æ˜¯éœ€è¦åŠ ä¸Šä¸€ä¸ª load_model çš„æ­¥éª¤

load_model ä¸­å·²ç»é»˜è®¤å°†æ¨¡å‹è®¾ç½®ä¸º chatglm-6b-int4

å…¶å®ä¹Ÿå¯ä»¥å°†æ¨¡å‹åŠ è½½æ”¾åˆ° init çš„éƒ¨åˆ†

```python
llm = OpenAI()

llm = ChatGLM()
llm.load_model()
```

è°ƒç”¨éœ€è¦ä½¿ç”¨ _call å‡½æ•°

åŒæ—¶åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œæ¯æ¬¡è°ƒç”¨å‡½æ•°éƒ½ä¼šå°† history è®°å½•åˆ° ChatGLM ç±»

```python
for resp, history in llm._call("ä½ å¥½", streaming=False):
    print(resp)
    print(history)
    
ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM-6Bï¼Œæ˜¯æ¸…åå¤§å­¦KEGå®éªŒå®¤å’Œæ™ºè°±AIå…¬å¸äº2023å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯æœåŠ¡å¹¶å¸®åŠ©äººç±»ï¼Œä½†æˆ‘å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„äººã€‚
[['ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'], ['è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM-6Bï¼Œæ˜¯æ¸…åå¤§å­¦KEGå®éªŒå®¤å’Œæ™ºè°±AIå…¬å¸äº2023å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯æœåŠ¡å¹¶å¸®åŠ©äººç±»ï¼Œä½†æˆ‘å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„äººã€‚'], ['è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM-6Bï¼Œæ˜¯æ¸…åå¤§å­¦KEGå®éªŒå®¤å’Œæ™ºè°±AIå…¬å¸äº2023å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯æœåŠ¡å¹¶å¸®åŠ©äººç±»ï¼Œä½†æˆ‘å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„äººã€‚']]
```

## Todo:

æ¨¡å‹æ˜¾å­˜å›æ”¶

å†å²è®°å½•

æ›´æ–° chain

## 8. Document QA è¯¦ç»†è¯´æ˜

è¿™éƒ¨åˆ†æ˜¯å¯¹ simplepdf çš„è¡¥å……ï¼ŒåŸºäºå®˜æ–¹æ–‡æ¡£ï¼Œå¯¹æ•´ä¸ªè¯»å–æ–‡ä»¶è¿›è¡Œ QA çš„æµç¨‹åšè¯¦ç»†è¯´æ˜

https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html

### 8.1 Indexes æ€»ä»‹ç»

LangChain åŸºäº index å¯¹ documents åšåŒ¹é…

æœ€åŸºæœ¬çš„æ¥å£å°±æ˜¯ retrieverï¼Œæ ¹æ® query æ‰¾åˆ°æœ€ç›¸å…³çš„ k ä¸ª documens

å¯¹äºéç»“æ„åŒ–æ•°æ®ï¼Œä¾‹å¦‚æ–‡æ¡£ï¼Œå¯ä»¥ç”¨ index å’Œ retrieve æ¥ç´¢å¼•

è€Œå¯¹äºç»“æ„åŒ–æ•°æ®ï¼Œä¾‹å¦‚ csvï¼Œæœ‰ç›¸åº”æ•°æ®ç±»å‹çš„ç´¢å¼•å‡½æ•°

ä¸»è¦çš„éƒ¨åˆ†ï¼š

Document Loaders

Text Splitters

Vector Stores

Retrievers

### 8.2 Document Loaders

https://python.langchain.com/en/latest/modules/indexes/document_loaders.html

è¯»å–æ”¯æŒå„ç§å„æ ·çš„æ–‡ä»¶æ ¼å¼

æ ¼å¼åŒ–æ–‡ä»¶ï¼šjsonï¼Œcsvï¼Œdataframeï¼Œdirectoryï¼Œmarkdown

å¤šåª’ä½“ï¼šHTMLï¼Œimagesï¼Œpdfï¼ŒPPT

å…¶ä»–ï¼šbilibiliï¼Œdiscordï¼Œemailï¼Œgitï¼Œgitbookï¼Œyoutube



å¯¹äºç»“æ„æ•°æ®ï¼Œåƒ csvã€dataframeã€æ•°æ®åº“ï¼Œä¸€è¡Œå°±æ˜¯ä¸€ä¸ª Documentï¼Œpage_content ä¿å­˜è¡Œçš„ä¸»é”®ï¼Œmetadata ä¿å­˜æ¯ä¸€è¡Œçš„å„ä¸ªå€¼

å¯¹äºæ–‡æ¡£æ–‡ä»¶ï¼Œåƒ HTMLã€markdownã€PDFï¼Œéœ€è¦åˆ†å—æˆå¤šä¸ªæ®µè½ï¼Œæ¯ä¸ªæ®µè½å°±æ˜¯ä¸€ä¸ª Document

å¯¹äºéç»“æ„åŒ–æ•°æ®ï¼Œåƒ å›¾ç‰‡ã€Youtube é“¾æ¥ï¼Œè¿˜æ²¡å¼„æ˜ç™½æ€ä¹ˆåŠ è½½



å®˜æ–¹é¢„è®¾äº†å¤šç§ Loaderï¼Œæ¯ä¸ª Loader æ•´åˆäº†ç¬¬ä¸‰æ–¹çš„åº“ï¼Œä¾‹å¦‚ BS4ã€PyPDF ç­‰ï¼Œéœ€è¦å¦å¤–å®‰è£…å¯¹åº”çš„åº“ã€‚å¯ä»¥è‡ªå·±ä½¿ç”¨è¿™äº›åº“è¯»å–å¥½æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨ Loader åŠ è½½

### 8.3 Text Splitter

å¦‚æœç”¨ Loader åŠ è½½ç»“æ„åŒ–æ•°æ®ï¼Œä¸€è¡Œä¸€ä¸ª Docï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±è¿›è¡Œåˆ†æ®µ

ä½†æ˜¯å¦‚æœè¯»å–éç»“æ„åŒ–æ•°æ®ï¼Œä¾‹å¦‚ä¸€ä¸ª 100 é¡µçš„ PDFï¼Œæˆ‘ä»¬ä¸å¯èƒ½æŠŠå…¨éƒ¨å†…å®¹ä¸€æ¬¡æ€§ä¸¢ç»™ LLM



å› æ­¤æˆ‘ä»¬å¸Œæœ›æ¯ä¸€ä¸ª Doc ä¿å­˜å…¶ä¸­å‡ é¡µçš„å†…å®¹ï¼Œåœ¨ç”¨æˆ·è¿›è¡Œæé—®çš„æ—¶å€™æ‰¾åˆ°æœ€ç›¸å…³çš„å‡ ä¸ª Docsï¼Œè®©æ¨¡å‹æ ¹æ®è¿™å‡ æ®µå†…å®¹è¿›è¡Œå›ç­”

ç²—æš´çš„åˆ’åˆ†æ–¹å¼æ˜¯ï¼Œç›´æ¥æŒ‰ç…§é•¿åº¦å°†æ–‡ç« åˆ‡å¼€ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯æ–­å¼€ã€‚ç†æƒ³çš„åˆ’åˆ†æ–¹å¼æ˜¯ï¼Œå°†ç›¸å…³çš„å†…å®¹åˆ’åˆ†åˆ°åŒä¸€ä¸ª Doc é‡Œï¼Œä¿æŒä¸Šä¸‹æ–‡çš„å®Œæ•´æ€§ï¼Œä½†æ˜¯éœ€è¦å¯¹æ–‡ç« çš„è¯­ä¹‰åšç†è§£

ï¼ˆçœ‹ LangChain å®˜æ–¹æ–‡æ¡£ä¼¼ä¹ä¹Ÿæ²¡æœ‰æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œåˆ†å‰²ï¼Œåªæ˜¯åšäº†ç®€å•çš„é•¿åº¦åˆ†æ®µï¼‰



ç¬¬ä¸€ç§åˆ’åˆ†æ–¹å¼ï¼ŒæŒ‰ç…§é•¿åº¦è¿›è¡Œåˆ’åˆ†ã€‚å°†å…¨ç¯‡æ–‡æ¡£æ‹†æˆä¸€å¥ä¸€å¥è¯ï¼Œæ¯æ¬¡é€‰æ‹© N å¥è¯ç»„æˆä¸€æ®µï¼Œæ¯æ®µçš„æ€»å­—æ•°ä¸è¶…è¿‡ max_len

ç¬¬äºŒç§åˆ’åˆ†æ–¹å¼ï¼ŒæŒ‰ç…§æ–‡æ¡£ä¿¡æ¯è¿›è¡Œåˆ’åˆ†ã€‚å¦‚æœæ–‡æ¡£è‡ªå·±æœ‰ç« èŠ‚ä¿¡æ¯ï¼Œåˆ™æ‰¾åˆ°é‡Œé¢çš„ç« èŠ‚è¿›è¡Œåˆ’åˆ†

ç¬¬ä¸‰ç§åˆ’åˆ†æ–¹å¼ï¼Œä½¿ç”¨ Text Splitter è¿›è¡Œåˆ’åˆ†ï¼š

ï¼ˆæŒ‰ç…§çŸ¥ä¹è¯„è®ºï¼Œé»˜è®¤ Splitter å¯¹ä¸­æ–‡çš„ç†è§£èƒ½åŠ›ä¸å¥½ã€‚å¦‚æœé‡‡ç”¨ Splitter æ–¹å¼éœ€è¦å¦å¤–æ‰¾ä¸­æ–‡é€‚é…çš„æ¨¡å‹ã€‚ä¸çŸ¥é“æŒ‰é•¿åº¦åˆ’åˆ†æ–¹å¼å’ŒæŒ‰ Splitter åˆ’åˆ†æ–¹å¼çš„æ•ˆæœå·®åˆ«æœ‰å¤šå¤§ï¼‰

LangChain é‡Œæ•´åˆäº†å¤šç§ Text Splitterï¼Œå¯¹æ–‡ç« è¿›è¡Œåˆ†æ®µï¼Œæœ‰ chunk_size/chunk_overlap ç­‰å‚æ•°

```python
from langchain.text_splitter import CharacterTextSplitter

with open("../../state_of_the_union.txt") as f:
    state_of_the_union = f.read()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)

docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{"source": str(i)} for i in range(len(texts))]).as_retriever()
```

 é»˜è®¤çš„é€‰é¡¹å°±ç®— RecursiveCharacterTextSplitter

æ ¹æ®æ¢è¡Œ '\\n', '\\n\\n' ç­‰ï¼Œå°†æ–‡ç« åˆ†æ®µï¼Œå‡è®¾ chunk_size = 100ï¼Œchunk_overlap = 10

å¦‚æœä¸€ä¸ªæ®µå°äº chunk å¤§å°ï¼Œåˆ™åœ¨ä¸€ä¸ª chunk ä¸­è£…ä¸‹å°½å¯èƒ½å¤šçš„æ®µã€‚ä¾‹å¦‚ä¸‰ä¸ªæ®µ 30/65/20ï¼Œchunk1 = æ®µ 1+2ï¼Œchunk2 = æ®µ 3 

å¦‚æœä¸€ä¸ªæ®µå¤§äº chunk å¤§å°ï¼Œé‚£å°±åˆ†æˆä¸¤å—ï¼Œåé¢ chunk çš„å¼€å¤´ç­‰äºå‰é¢ chunk çš„ç»“å°¾

```python
# åŸæ–‡
'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\n'

# ç¬¬ä¸€æ®µå¤ªé•¿ï¼Œæ‹†åˆ†æˆä¸¤ä¸ª chunk
Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and', metadata={})
Document(page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.', metadata={})
# ç¬¬äºŒæ®µå®Œæ•´æ”¾è¿› chunk é‡Œ
Document(page_content='Last year COVID-19 kept us apart. This year we are finally together again.', metadata={})
```

å¯¹äº chunk é•¿åº¦è®¡ç®—ï¼Œå¯ä»¥ç®€å•ä½¿ç”¨å­—ç¬¦æ•°ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ huggingface_tokenizer

å¯¹äº Latexï¼Œæœ‰ LatexTextSplitterï¼Œå¯ä»¥æ ¹æ® \\section \\subsection ä¹‹ç±»çš„æ ‡è¯†ç¬¦æ¥åˆ’åˆ†

å¯¹äº Markdownï¼Œæœ‰ MarkdownTextSplitterï¼Œæ ¹æ® # ## ç­‰åˆ’åˆ†

è¿˜æœ‰åƒ Python ä»£ç ç­‰ç±»å‹ï¼Œä¹Ÿæœ‰æ™ºèƒ½åˆ’åˆ†å·¥å…·

### 8.4 åˆ›å»º Index

ä½¿ç”¨é¢„è®¾ Loader åŠ è½½æ•°æ®ï¼Œå¦‚æœä¸éœ€è¦æ‰‹åŠ¨åˆ’åˆ†ï¼Œå¯ä»¥åˆ›å»ºå¯¹åº”çš„ç´¢å¼•

è®© query åœ¨ç´¢å¼•é‡Œè¿›è¡ŒåŒ¹é…

```python
from langchain.indexes import VectorstoreIndexCreator
index = VectorstoreIndexCreator().from_loaders([loader])

query = "What did the president say about Ketanji Brown Jackson"
index.query(query)
```

### 8.5 åˆ›å»º Chain

ä¸¤ç§é€‰é¡¹ï¼š

with Sourcesï¼šåœ¨å›ç­”é‡Œé™„ä¸Šæ¥æº Documentï¼Œä¼¼ä¹æ˜¯ ChatModel

Retrievalï¼šåŸºäº VectorDBï¼Œä½†æ˜¯æ²¡ææ‡‚æœ‰ä»€ä¹ˆä¸åŒ

```python
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff")

from langchain.chains.qa_with_sources import load_qa_with_sources_chain
chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type="stuff")

from langchain.chains import RetrievalQA
qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=docsearch.as_retriever())

from langchain.chains import RetrievalQAWithSourcesChain
chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type="stuff", retriever=docsearch.as_retriever())
```

é€‰æ‹© chain å¹¶è¿è¡Œ

```python
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm, chain_type="stuff")
chain.run(input_documents=docs, question=query)

from langchain.chains.qa_with_sources import load_qa_with_sources_chain
chain = load_qa_with_sources_chain(llm, chain_type="stuff")
chain({"input_documents": docs, "question": query}, return_only_outputs=True)
```

### 8.6 æœ¬åœ° Embedding

å’Œ word embedding ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª text embedding åšæ•´æ®µçš„æ–‡æœ¬å½’çº³

Massive Text Embedding Benchmark (MTEB) Leaderboard

https://huggingface.co/spaces/mteb/leaderboard 

æ’è¡Œæ¦œå¤§éƒ¨åˆ†æ˜¯è‹±æ–‡ä¸ºä¸»ï¼Œä¸­æ–‡ä¹Ÿå¯ä»¥åšä½†æ˜¯æ•ˆæœæ²¡è¿™ä¹ˆå¥½

ä¸‹é¢æ˜¯ä¸­æ–‡çš„æ¨¡å‹ï¼Œç›®å‰åªæ‰¾åˆ°è¿™ä¸ª

https://huggingface.co/shibing624/text2vec-base-chinese

#### text2vec æµ‹è¯•

è¿™ä¸€æ­¥å¥½åƒä¸æ˜¯å¿…è¦çš„

```python
pip install -U text2vec
```

é¦–å…ˆå°†æ•´ä¸ªé¡¹ç›®å’Œæ¨¡å‹ä¸‹è½½ä¸‹æ¥ï¼Œç”¨ text2vec ç®€å•æµ‹è¯•

åˆ›å»ºå››ä¸ªå¥å­ï¼Œå‰ä¸‰ä¸ªç›¸å¯¹æ¥è¿‘ï¼Œæœ€åä¸€ä¸ªæ— å…³

å˜æˆå››ä¸ª embeddings

```python
from text2vec import SentenceModel

model = SentenceModel(r'D:\Projects\text2vec-base-chinese')
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡', 'å¦‚ä½•åŠç†é“¶è¡Œå¡', 'å“ˆé‡Œè·¯å¤§æ—‹é£']
embeddings = model.encode(sentences)
print(embeddings)
```

è®©åè®© GPT é…±å¸®æˆ‘å¯è§†åŒ–å››ä¸ª embeddings çš„å·®å¼‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
# PCA
pca = PCA(n_components=2)
embeddings_pca = pca.fit_transform(embeddings)
# t-SNE with perplexity=2
tsne = TSNE(n_components=2, random_state=0, perplexity=2)
embeddings_tsne = tsne.fit_transform(embeddings)
```

![](embedding_visualize.png)

#### ç»“åˆ LangChain æµ‹è¯•

è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œæˆ–è€…å°†æ¨¡å‹ä¸‹è½½ä¸‹æ¥æ”¾åˆ°æœ¬åœ°

ç„¶åæŠŠ OpenAI Embeddings æ”¹æˆ HuggingFace Embeddings

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

embedding = HuggingFaceEmbeddings(model_name=model_name='shibing624/text2vec-base-chinese') # è‡ªåŠ¨ä¸‹è½½
embedding = HuggingFaceEmbeddings(model_name=r'D:\Projects\text2vec-base-chinese') # æ‰‹åŠ¨ä¸‹è½½æŒ‡å®šç›®å½•
```

æœ‰ä¸ªå¥‡æ€ªçš„æŠ¥é”™ï¼Œä¸çŸ¥é“æ€ä¹ˆè§£å†³ï¼Œä½†æ˜¯ä¸å½±å“æ¨¡å‹åŠ è½½ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨

```
No sentence-transformers model found with name D:\Projects\text2vec-base-chinese. Creating a new one with MEAN pooling.
```

æœ‰ä¸€ä¸ªå°å°çš„å‘ï¼Œæ¢ embedding æ¨¡å‹ä¹‹å db æ–‡ä»¶å¤¹è¦æ¢ä¸€ä¸‹

å¦‚æœç”¨ HF Embedding å†™å…¥ OpenAI Embedding çš„æ•°æ®åº“æ–‡ä»¶ä¼šå› ä¸ºç»´åº¦ä¸åŒ¹é…æŠ¥é”™

éœ€è¦é‡æ–°å¼€ä¸€ä¸ª db æ–‡ä»¶å¤¹

### 8.7 Chat Models

```python
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
```

ç¼–å†™ prompt éœ€è¦æä¾› SystemMessage å’Œ HumanMessage

```python
prompt=PromptTemplate(
    template="You are a helpful assistant that translates {input_language} to {output_language}.",
    input_variables=["input_language", "output_language"],
)
system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)

human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
# get a chat completion from the formatted messages
chat(chat_prompt.format_prompt(input_language="English", output_language="French", text="I love programming.").to_messages())
```

è¿˜å¯ä»¥å‘ prompt ä¸­æä¾› few shot ä¾‹å­

```
example_human = HumanMessagePromptTemplate.from_template("Hi")
example_ai = AIMessagePromptTemplate.from_template("Argh me mateys")

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])
```

### 8.8 Custom LLM

ç”±äº LangChain æ²¡æœ‰æ•´åˆ ChatGLMï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è‡ªå·±ç¼–å†™ ChatGLM ç±»

ç›®æ ‡æ˜¯åƒä½¿ç”¨ OpenAI æ¥å£ä¸€æ ·ä½¿ç”¨ ChatGLM æ¨¡å‹

https://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html

https://juejin.cn/post/7226157821708681277

https://zhuanlan.zhihu.com/p/624240080

åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ LLM çš„æ¨¡æ¿æ˜¯è¿™æ ·çš„ï¼š

```python
from typing import Any, List, Mapping, Optional

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM

class CustomLLM(LLM):
    
    n: int
        
    @property
    def _llm_type(self) -> str:
        return "custom"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        return prompt[:self.n]
    
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"n": self.n}
```

åªæœ‰ _call å‡½æ•°æ˜¯å¿…é¡»è¦å®šä¹‰çš„ã€‚é€šè¿‡å°†å­—ç¬¦ä¸²è¾“å…¥ _callï¼Œè¿”å›è¾“å‡ºç»“æœ

åŠ è½½æ¨¡å‹åˆ™éœ€è¦å†™å…¥ init æ–¹æ³•æˆ–è€…å•ç‹¬åˆ›å»ºä¸€ä¸ªå‡½æ•°

è¿˜æœ‰ _llm_type, _identifying_params ç­‰æ–¹æ³•å¯ä»¥å®šä¹‰



åœ¨ä½¿ç”¨ llm('Hello') çš„æ—¶å€™

é¦–å…ˆä¼šè°ƒç”¨ _\_call\_\_ï¼Œç„¶åè°ƒç”¨ generateï¼Œç„¶åè°ƒç”¨ _generateï¼Œç„¶åè°ƒç”¨ _call

å¯¹äº LLM æ¥è¯´ï¼Œ\_\_call\_\_ å‡½æ•°å†³å®šäº†è¿”å›å€¼æ˜¯ str

```python
# langchain\llms\base.py
class BaseLLM(BaseLanguageModel, ABC):
# 276
    def __call__(
        self, prompt: str, stop: Optional[List[str]] = None, callbacks: Callbacks = None
    ) -> str:
        """Check Cache and run the LLM on the given prompt and input."""
        return (
            self.generate([prompt], stop=stop, callbacks=callbacks)
            .generations[0][0]
            .text
        )
# 170
    def generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
    ) -> LLMResult:
        # ...
        if langchain.llm_cache is None or disregard_cache:
            # ...
            output = self._generate(prompts, stop=stop, run_manager=run_manager)
# 365
    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> LLMResult:
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop, run_manager=run_manager)
            generations.append([Generation(text=text)])
```

è€Œå¯¹äº ChatModelï¼Œ\_\_call\_\_ è¿”å›çš„æ˜¯ BaseMessage

```python
# langchain\chat_models\base.py
# 169
    def __call__(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
    ) -> BaseMessage:
        generation = self.generate(
            [messages], stop=stop, callbacks=callbacks
        ).generations[0][0]
        if isinstance(generation, ChatGeneration):
            return generation.message
        else:
            raise ValueError("Unexpected generation type")
```





## 9. è‰ç¨¿åŒº

LangChain + ChatGLM

https://zhuanlan.zhihu.com/p/623004492

https://zhuanlan.zhihu.com/p/622717995

æ¶æ„å›¾

https://zhuanlan.zhihu.com/p/613842066

Prompt æ³¨å…¥/ä¸­é—´äººæ”»å‡»

https://zhuanlan.zhihu.com/p/624139892

https://zhuanlan.zhihu.com/p/624584889

## 10. Promptï¼ˆå¾…è¡¥å……ï¼‰

å¯¹ QA æœºå™¨äººçš„ prompt ç®¡ç†ï¼Œå¾ˆé‡è¦

https://github.com/dair-ai/Prompt-Engineering-Guide

å¯¹äº Tool éœ€è¦æ–‡å­—ç›¸å…³çš„è¯´æ˜

å¯¹äº Agent éœ€è¦æµç¨‹æŒ‡å¼•

å¯¹äºæ–‡æœ¬æ±‡æ€»éœ€è¦è®©æ¨¡å‹åœ¨é™å®šèŒƒå›´å†…å›ç­”é—®é¢˜

