# ChatGLM éƒ¨ç½²

## 1. å‡†å¤‡å·¥ä½œ

### 1.1 CUDA & CuDNN

CUDA å®‰è£…

https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#removing-cuda-toolkit-and-driver

https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local

CuDNN å®‰è£…

https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify

https://developer.nvidia.cn/rdp/cudnn-download

### 1.2 Python ç¯å¢ƒ

æ–°å»º conda ç¯å¢ƒï¼ˆPython 3.8 or 3.9ï¼‰

```shell
conda create -n chat python=3.8
```

å®‰è£… pytorch

```shell
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
```

å…‹éš† ChatGLM-6b é¡¹ç›®

```shell
git clone https://github.com/THUDM/ChatGLM-6B.git
cd ChatGLM-6B
```

å®‰è£…ä¾èµ–

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt
```

## 2. å¯¼å…¥æ¨¡å‹

å…‹éš† hugging face çš„ chatglm-6b é¡¹ç›®

```shell
# ä¸å«æ¨¡å‹å‚æ•°
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b
```

ä¸‹è½½ FP16 çš„æ¨¡å‹å‚æ•°ï¼Œè¦†ç›– chatglm-6b ä¸­çš„æ–‡ä»¶

https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b&mode=list



å…‹éš† hugging face çš„ chatglm-6b-int4 é¡¹ç›®

```shell
# ä¸å«æ¨¡å‹å‚æ•°
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b-int4
```

ä¸‹è½½ INT4 çš„æ¨¡å‹å‚æ•°ï¼Œè¦†ç›– chatglm-6b-int4 ä¸­çš„æ–‡ä»¶

https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b-int4&mode=list



å…‹éš†çš„æ¨¡å‹æ–‡ä»¶å¤¹å»ºè®®ä¿å­˜åˆ° ChatGLM-6b é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹

ä¾‹å¦‚ ChatGLM-6b/chatglm-6b å’Œ ChatGLM-6b/chatglm-6b-int4 

## 3. è¿è¡Œ test.py(CPU)

å‚è€ƒ GitHub å®˜æ–¹æ–‡æ¡£â€œä»£ç è°ƒç”¨â€ç« èŠ‚è¿›è¡Œæµ‹è¯•

tokenizer å’Œ model ä¸­çš„å‚æ•°ï¼Œéœ€è¦å¡«å†™æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹çš„ç›®å½•

åªä½¿ç”¨ CPU æ¨ç†ï¼Œé€Ÿåº¦éå¸¸æ…¢

```python
from transformers import AutoTokenizer, AutoModel
import time

start_time = time.time()
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("chatglm-6b-int4",trust_remote_code=True).float()
model = model.eval()
print('Load:',time.time()-start_time)


qlist = ['ä½ å¥½ï¼', 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚', 'ä½ æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ']
history = []
for query in qlist:
	print(query)
	start_time = time.time()
	response, history = model.chat(tokenizer, query, history=history)
	print(response)
	print('Time:',time.time()-start_time)
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\2.png)

## 4. è¿è¡Œ test.py(GPU)

ï¼ˆåœ¨æˆåŠŸè¿è¡Œä¹‹å‰é‡åˆ°çš„ CUDA ç›¸å…³é”™è¯¯æ”¾åœ¨åæ–‡éƒ¨åˆ†ï¼Œéœ€è¦å…ˆæ’é™¤é—®é¢˜å†è¿è¡Œ GPU æµ‹è¯•ï¼‰

ä¿®æ”¹ test.py å¦‚ä¸‹

åœ¨ AutoModel åŠ è½½æ—¶ä½¿ç”¨äº† .half().cuda()ï¼Œå°†æ¨¡å‹åŠ è½½åˆ° GPU ä¸Š

åŒæ—¶å¢åŠ äº† revision='int4'ï¼Œè¯´æ˜æ¨¡å‹ç‰ˆæœ¬ï¼Œç‰ˆæœ¬åå¯ä»¥éšæ„å¡«å†™

```python
from transformers import AutoTokenizer, AutoModel
import time

start_time = time.time()
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
model = AutoModel.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4').half().cuda()
model = model.eval()
print('Load:',time.time()-start_time)

qlist = ['ä½ å¥½ï¼', 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚', 'ä½ æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ', 'è¯·ä»æ•´ä½“ä»‹ç»ä¸€ä¸‹ç¯ä¿è¡Œ
ä¸šã€‚', 'è¯·å†™ä¸€æ®µä½¿ç”¨pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ã€‚']
history = []
for query in qlist:
    print(query)
    start_time = time.time()
    response, history = model.chat(tokenizer, query, history=history)
    print(response)
    print('Time:',time.time()-start_time)
```



````shell
(chat) root@ubuntu-virtual-machine:/data1/chat_model# python test.py
----------------------------------------------
Cannot load cpu kernel, don't use quantized model on cpu.
Using quantization cache
Applying quantization to glm layers
Load: 21.275478839874268
ä½ å¥½ï¼
The dtype of attention mask (torch.int64) is not bool
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
Time: 11.327528953552246
è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚
æˆ‘å¯ä»¥å›ç­”å…³äºæˆ‘è‡ªå·±çš„ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚æˆ‘çš„è®­ç»ƒæ•°æ®é›†ï¼Œæˆ‘çš„ç®—æ³•æ¨¡å‹å’Œè¿è¡Œæ–¹å¼ç­‰ç­‰ã€‚åŒæ—¶ï¼Œæˆ‘ä¹Ÿå¯ä»¥æä¾›ä¸€äº›å…³äºæˆ‘çš„ä¸»äºº ChatGLM-6B çš„ä¿¡æ¯ï¼Œå¦‚æœå¯¹å¥¹æ„Ÿå…´è¶£çš„è¯ã€‚
Time: 9.367414236068726
ä½ æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ
ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘å…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæˆ‘å¯ä»¥ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œè¯­éŸ³ã€‚

2. çŸ¥è¯†åº“ï¼šæˆ‘å¯ä»¥è®¿é—®å’Œå­¦ä¹ å¤§é‡çš„æ–‡æœ¬å’Œæ•°æ®é›†ï¼Œä»¥ä¾¿å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

3. è¯­è¨€ç”Ÿæˆï¼šæˆ‘å¯ä»¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œä¾‹å¦‚æ–‡ç« ã€å›å¤å’Œå¯¹è¯ã€‚

4. è‡ªåŠ¨æ‘˜è¦ï¼šæˆ‘å¯ä»¥è‡ªåŠ¨æå–æ–‡æœ¬ä¸­çš„é‡ç‚¹å’Œæ‘˜è¦ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥å¿«é€Ÿäº†è§£æ–‡æœ¬å†…å®¹ã€‚

5. å¯¹è¯ï¼šæˆ‘å¯ä»¥ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œå›ç­”ä»–ä»¬çš„é—®é¢˜æˆ–æ‰§è¡Œå…¶ä»–ä»»åŠ¡ï¼Œä¾‹å¦‚æä¾›å»ºè®®æˆ–æ‰§è¡Œè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚

å¸Œæœ›è¿™äº›æ¦‚æ‹¬å¯ä»¥æä¾›å¸®åŠ©ï¼
Time: 28.96828293800354
è¯·ä»æ•´ä½“ä»‹ç»ä¸€ä¸‹ç¯ä¿è¡Œä¸šã€‚
ç¯ä¿è¡Œä¸šæ˜¯æŒ‡ä¸“é—¨ä»äº‹ç¯å¢ƒä¿æŠ¤å’Œæ¸…æ´å‘å±•çš„è¡Œä¸šï¼Œæ—¨åœ¨é€šè¿‡è§£å†³ç¯å¢ƒé—®é¢˜æ¥æ”¹å–„ç¯å¢ƒè´¨é‡ï¼Œæé«˜å¯æŒç»­å‘å±•æ°´å¹³ã€‚ç¯ä¿è¡Œä¸šæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ç¯ä¿æŠ€æœ¯ã€ç¯å¢ƒç›‘æµ‹ã€ç¯å¢ƒæ²»ç†ã€ç¯ä¿ææ–™ã€æ¸…æ´æœåŠ¡ã€ç¯ä¿æ³•è§„ç­‰ã€‚

åœ¨ç¯ä¿è¡Œä¸šä¸­ï¼Œç¯ä¿æŠ€æœ¯æ˜¯æŒ‡åˆ©ç”¨å„ç§æŠ€æœ¯å’Œæ–¹æ³•æ¥å‡å°‘ç¯å¢ƒæ±¡æŸ“å’Œå¤„ç†æ±¡æŸ“ç‰©çš„æ–¹æ³•ã€‚ç¯å¢ƒç›‘æµ‹æ˜¯æŒ‡å¯¹ç¯å¢ƒæ±¡æŸ“å’Œæ°´è´¨è¿›è¡Œæ£€æµ‹å’Œåˆ†æï¼Œä»¥ä¾¿è¯†åˆ«æ±¡æŸ“ç‰©å¹¶é‡‡å–ç›¸åº”çš„æªæ–½ã€‚ç¯å¢ƒæ²»ç†æ˜¯æŒ‡å¯¹ç¯å¢ƒæ±¡æŸ“è¿›è¡Œæ²»ç†ï¼ŒåŒ…æ‹¬å¯¹æ±¡æŸ“ç‰©è¿›è¡Œå›æ”¶ã€å¤„ç†å’Œè½¬åŒ–ã€‚

ç¯ä¿ææ–™æ˜¯æŒ‡ç”¨äºç¯ä¿é¢†åŸŸçš„ææ–™ï¼Œè¿™äº›ææ–™å¯ä»¥ç”¨äºåˆ¶é€ ç¯ä¿äº§å“ï¼Œå‡å°‘å¯¹ç¯å¢ƒçš„æ±¡æŸ“ã€‚æ¸…æ´æœåŠ¡æ˜¯æŒ‡æä¾›æ¸…æ´æœåŠ¡çš„å…¬å¸ï¼ŒåŒ…æ‹¬å®¶åº­æ¸…æ´ã€å•†ä¸šæ¸…æ´å’Œå·¥ä¸šæ¸…æ´ç­‰ã€‚

ç¯ä¿æ³•è§„æ˜¯æŒ‡æœ‰å…³ç¯å¢ƒä¿æŠ¤çš„æ³•å¾‹å’Œæ”¿ç­–ï¼Œæ—¨åœ¨ä¿æŠ¤ç¯å¢ƒå’Œä¿ƒè¿›å¯æŒç»­å‘å±•ã€‚è¿™äº›æ³•è§„æ—¨åœ¨å‡å°‘ç¯å¢ƒæ±¡æŸ“ã€ä¿æŠ¤ç”Ÿæ€å¹³è¡¡ã€åŠ å¼ºç¯å¢ƒç›‘ç®¡å’Œæ¨åŠ¨ç¯ä¿æŠ€æœ¯çš„åº”ç”¨ã€‚

ç¯ä¿è¡Œä¸šæ˜¯ä¸€ä¸ªæ¶‰åŠä¼—å¤šé¢†åŸŸï¼Œæ—¨åœ¨ä¿æŠ¤ç¯å¢ƒå’Œæé«˜å¯æŒç»­å‘å±•æ°´å¹³çš„è¡Œä¸šã€‚åœ¨æ—¥ç›Šé‡è§†ç¯å¢ƒä¿æŠ¤çš„å½“ä»Šä¸–ç•Œï¼Œç¯ä¿è¡Œä¸šçš„å‰æ™¯çœ‹å¥½ï¼ŒåŒæ—¶ä¹Ÿé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚
Time: 46.70101070404053
è¯·å†™ä¸€æ®µä½¿ç”¨pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ã€‚
ä»¥ä¸‹æ˜¯ä½¿ç”¨Pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰æ•°æ®
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# ä½¿ç”¨PCAè¿›è¡Œé™ç»´
pca = np.random.randn(data.shape[0], data.shape[1]+1)
pca[:,1] = data

# æ‰“å°PCAç»“æœ
print("PCAç»“æœï¼š")
print(pca.shape)

# å¯è§†åŒ–PCAç»“æœ
data_pca = pca.reshape((data.shape[0], data.shape[1]+1))
plt.imshow(data_pca, cmap='gray', aspect='auto', extent=[0, 10, 0, 10])
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Pca é™ç»´ç»“æœ')
plt.show()
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œå°†æ•°æ®ç»è¿‡PCAé™ç»´åå­˜å‚¨åœ¨å˜é‡ `data_pca` ä¸­ï¼Œç„¶åä½¿ç”¨ `imshow` å‡½æ•°å°†é™ç»´åçš„æ•°æ®å’ŒPCAç»“æœå¯è§†åŒ–ã€‚
Time: 60.72647047042847
````



è¿è¡Œæƒ…å†µ

| æ¨¡å‹ | å†…å­˜(G)   | æ˜¾å­˜(G) | åŠ è½½(s) | çŸ­å¥(s) | é•¿æ®µ(s) |
| ---- | --------- | ------- | ------- | ------- | ------- |
| int4 | 6.5/8.5   | 5       | 15-20   | 3-10    | 30-60   |
| int8 | 14.5      | 7-9     | 100-120 | 10-20   | 30-100  |
| 6b   | 13.6/14.5 | 12-13   | 140-250 | 3-10    | 10-40   |

## 5. ä¿®å¤ CUDA åº“

### 5.1 é—®é¢˜æè¿°

ç¡®è®¤ cuda å¯ç”¨

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\3.png)

ä»¥ GPU æ¨¡å¼è¿è¡Œ test.py

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("chatglm-6b-int4",trust_remote_code=True).half().cuda()
model = model.eval()

print("First:")
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
print("Second:")
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", history=history)
print(response)
```

æ¨¡å‹è¿è¡Œæ—¶å‡ºç°æŠ¥é”™ï¼š

```shell
File "/root/anaconda3/envs/chat/lib/python3.8/site-packages/cpm_kernels/kernels/base.py", line 24, in get_module
    self._module[curr_device] = cuda.cuModuleLoadData(self._code)
  File "/root/anaconda3/envs/chat/lib/python3.8/site-packages/cpm_kernels/library/base.py", line 72, in wrapper
    raise RuntimeError("Library %s is not initialized" % self.__name)
RuntimeError: Library cuda is not initialized
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\4.png)

å®Œæ•´æŠ¥é”™ï¼š

```shell
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
No compiled kernel found.
Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c
Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Setting CPU quantization kernel threads to 2
Parallel kernel is not recommended when parallel num < 4.
Using quantization cache
Applying quantization to glm layers
First:
The dtype of attention mask (torch.int64) is not bool
Traceback (most recent call last):
  File "/data1/chat_model/test.py", line 14, in <module>
    response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1286, in chat
    outputs = self.generate(**inputs, **gen_kwargs)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/transformers/generation/utils.py", line 1452, in generate
    return self.sample(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/transformers/generation/utils.py", line 2468, in sample
    outputs = self(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1191, in forward
    transformer_outputs = self.transformer(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 997, in forward
    layer_ret = layer(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 445, in forward
    mixed_raw_layer = self.query_key_value(hidden_states)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 375, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 53, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 274, in extract_weight_to_half
    func(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 48, in __call__
    func = self._prepare_func()
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 40, in _prepare_func
    self._module.get_module(), self._func_name
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 24, in get_module
    self._module[curr_device] = cuda.cuModuleLoadData(self._code)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/library/base.py", line 72, in wrapper
    raise RuntimeError("Library %s is not initialized" % self.__name)
RuntimeError: Library cuda is not initialized
```

### 5.2 é—®é¢˜å®šä½

/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/library/base.py

ä¿®æ”¹ elif sys.platform.startswith("linux") éƒ¨åˆ†ï¼Œåœ¨è°ƒç”¨ unix_find_lib çš„æ—¶å€™è¾“å‡ºæœç´¢çš„åº“å’Œè·¯å¾„ï¼ˆè§ # Edit here æ³¨é‡Šï¼‰

å‘ç°åœ¨æ¨¡å‹åŠ è½½å‰é¦–å…ˆä¼šæœç´¢ /usr/local/cuda/lib64ï¼Œå¯¼å…¥å››ä¸ª .so åº“

lib64 ä¸­ç¼ºå°‘äº† libcuda.soï¼Œå› æ­¤æŠ¥é”™

```python
def unix_find_lib(name):
    cuda_path = os.environ.get("CUDA_PATH", None)
    if cuda_path is not None:
        lib_name = os.path.join(cuda_path, "lib64", "lib%s.so" % name)
        if os.path.exists(lib_name):
            return lib_name

    cuda_path = "/usr/local/cuda"
    if cuda_path is not None:
        lib_name = os.path.join(cuda_path, "lib64", "lib%s.so" % name)
        if os.path.exists(lib_name):
            return lib_name

class Lib:
    def __init__(self, name):
        self.__name = name
        if sys.platform.startswith("win"):
            lib_path = windows_find_lib(self.__name)
            self.__lib_path = lib_path
            if lib_path is not None:
                self.__lib = ctypes.WinDLL(lib_path)
            else:
                self.__lib = None
        elif sys.platform.startswith("linux"):
            lib_path = unix_find_lib(self.__name)
            # Edit Here
            print(name, ':', lib_path)
            
            self.__lib_path = lib_path
            if lib_path is not None:
                self.__lib = ctypes.cdll.LoadLibrary(lib_path)
            else:
                self.__lib = None
        else:
            raise RuntimeError("Unknown platform: %s" % sys.platform)
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\6.png)

### 5.3 é—®é¢˜è§£å†³

é¦–å…ˆæœç´¢ç°æœ‰çš„ libcuda.so

ä½¿ç”¨ nvidia-smi æŸ¥çœ‹é©±åŠ¨ç‰ˆæœ¬æ˜¯ 515.65.01

å‘ç°åœ¨ /usr/lib/x86_64-linux-gnu ä¸­æœ‰ libcuda.so.515.65.01 å’Œ libcuda.so.525.105.17

```shell
(base) root@ubuntu-virtual-machine:~# find / -name "libcuda.*"
/usr/local/cuda-11.7/targets/x86_64-linux/lib/stubs/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so.1
/usr/lib/x86_64-linux-gnu/libcuda.so.525.105.17
/usr/lib/x86_64-linux-gnu/libcuda.so.515.65.01
/usr/lib/i386-linux-gnu/libcuda.so
/usr/lib/i386-linux-gnu/libcuda.so.1
/usr/lib/i386-linux-gnu/libcuda.so.515.65.01


(base) root@ubuntu-virtual-machine:~# ll /usr/lib/x86_64-linux-gnu/libcuda*
lrwxrwxrwx 1 root root       29 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcudadebugger.so.1 -> libcudadebugger.so.525.105.17
-rw-r--r-- 1 root root 10490248 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcudadebugger.so.525.105.17
lrwxrwxrwx 1 root root       12 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1
lrwxrwxrwx 1 root root       21 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.525.105.17
-rwxr-xr-x 1 root root 20988000 4æœˆ  27 10:31 /usr/lib/x86_64-linux-gnu/libcuda.so.515.65.01*
-rw-r--r-- 1 root root 29867944 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so.525.105.17
```

å°è¯•å°† /usr/lib/x86_64-linux-gnu/libcuda.so.515.65.01 å¤åˆ¶åˆ° /usr/local/cuda/lib64

ç”±äº /usr/lib/x86_64-linux-gnu/ ä¸­ä½¿ç”¨çš„æ˜¯ 525 ç‰ˆæœ¬é©±åŠ¨ï¼Œè€Œä¸æ˜¯ 515 ç‰ˆæœ¬

å¯¼è‡´é©±åŠ¨å’Œ cuda library ä¸ä¸€è‡´æŠ¥é”™ï¼Œåªèƒ½åˆ é™¤

ï¼ˆå¦‚æœé©±åŠ¨å’Œ cuda ä¸€è‡´çš„è¯ç›´æ¥å¤åˆ¶å³å¯ï¼‰

```shell
(chat) root@ubuntu-virtual-machine:/data1/chat_model# nvidia-smi
Failed to initialize NVML: Driver/library version mismatch
```

å› æ­¤éœ€è¦æ›´æ–° /usr/lib/x86_64-linux-gnu/ ä¸­çš„é©±åŠ¨

https://www.nvidia.com/Download/Find.aspx?lang=en-us

https://www.nvidia.com/download/driverResults.aspx/191320/en-us/

æ‰¾åˆ° CUDA åº“å¯¹åº”çš„ 515.65.01 ç‰ˆæœ¬é©±åŠ¨

ä¸‹è½½ sh NVIDIA-Linux-x86_64-515.65.01.run é‡æ–°å®‰è£…åï¼Œé©±åŠ¨ä¼šæ›´æ–°æˆ 515 ç‰ˆæœ¬

å¯ä»¥çœ‹åˆ° /usr/lib/x86_64-linux-gnu ä¸­ä½¿ç”¨çš„ libcuda.so å·²é“¾æ¥åˆ° libcuda.so.515.65.01*

ç„¶åå¤åˆ¶åˆ° /usr/local/cuda/lib64 å³å¯

ï¼ˆæ³¨æ„ ChatGLM æœç´¢çš„æ˜¯ libcuda.soï¼Œå°† libcuda.so.515.65.01* å¤åˆ¶è¿‡å»ä¹‹åéœ€è¦åˆ›å»ºè½¯é“¾æ¥æˆ–é‡å‘½åï¼Œè¿™é‡Œä¸è¿‡å¤šæ¼”ç¤ºï¼‰

```shell
(base) root@ubuntu-virtual-machine:/usr/lib/x86_64-linux-gnu# ll libcuda*
lrwxrwxrwx 1 root root       29 3æœˆ  27 19:40 libcudadebugger.so.1 -> libcudadebugger.so.525.105.17
-rw-r--r-- 1 root root 10490248 3æœˆ  27 19:40 libcudadebugger.so.525.105.17
lrwxrwxrwx 1 root root       12 4æœˆ  28 09:03 libcuda.so -> libcuda.so.1*
lrwxrwxrwx 1 root root       20 4æœˆ  28 09:03 libcuda.so.1 -> libcuda.so.515.65.01*
-rwxr-xr-x 1 root root 20988000 4æœˆ  28 09:03 libcuda.so.515.65.01*
```

## 6. æŠ¥é”™æç¤ºé—®é¢˜

### 6.1 revision æŠ¥é”™

è¿™ä¸ªæ˜¯æ²¡æœ‰æä¾›æ¨¡å‹ç‰ˆæœ¬å·çš„é—®é¢˜

åŠ å…¥ `revision` å‚æ•°è§£å†³ï¼Œç‰ˆæœ¬å·åç§°å¯ä»¥éšæ„å¡«å†™ï¼Œä¾‹å¦‚ 'int4'/'chatglm' ç­‰

```python
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
model = AutoModel.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
```

### 6.2 No compiled kernel found

åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µï¼Œé¦–å…ˆä¼šåŠ è½½é‡åŒ–ç”¨çš„ kernel

æ¨¡å‹åŠ è½½åˆ°å†…å­˜åä½¿ç”¨ kernel è¿›è¡Œé‡åŒ–ï¼Œå†åŠ è½½åˆ° GPU ä¸­



ä½†æ˜¯ç”±äº chatglm-6b-int4/quantization.py çš„é‡åŒ–éƒ¨åˆ†

kwargs ä¸ºç©ºï¼Œload_cpu_kernel æ²¡æœ‰å‚æ•°ä¼ è¿›æ¥

å› æ­¤æ¯æ¬¡åŠ è½½æ¨¡å‹éƒ½ä¼šç¼ºå¤± kernelï¼Œé‡æ–°ç¼–è¯‘

```shell
No compiled kernel found.
Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c
Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Setting CPU quantization kernel threads to 2
Parallel kernel is not recommended when parallel num < 4.
Using quantization cache
Applying quantization to glm layers
```

é¦–å…ˆæ ¹æ®æç¤ºï¼Œå°†ä¸´æ—¶åœ°å€ä¸­ç¼–è¯‘çš„ kernel å¤åˆ¶ä¸‹æ¥ï¼ˆLoad kernel æç¤ºçš„åœ°å€ï¼‰

å»ºè®®å¤åˆ¶åˆ° chatglm-6b-int4 æ¨¡å‹ç›®å½•

```shell
cp /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so ./chatglm-6b-int4/
```

ç„¶åä¿®æ”¹ chatglm-6b-int4/quantization.py çš„ 441 è¡Œï¼ˆè§ # Edit here æ³¨é‡Šï¼‰

```python
# 441
def quantize(model, weight_bit_width, use_quantization_cache=False, empty_init=False, **kwargs):
    """Replace fp16 linear with quantized linear"""

    query_key_value_quantization_cache = None
    dense_quantization_cache = None
    dense_h_to_4h_quantization_cache = None
    dense_4h_to_h_quantization_cache = None

    try:
        # Edit here
        # load_cpu_kernel(**kwargs)
        load_cpu_kernel(kernel_file='chatglm-6b-int4/quantization_kernels_parallel.so', **kwargs)
    except:
        if kernels is None:  # CUDA kernels failed
            print("Cannot load cpu or cuda kernel, quantization failed:")
            assert kernels is not None
        print("Cannot load cpu kernel, don't use quantized model on cpu.")
```

ä¼ å‚ä¸ºç©ºå¯¼è‡´å¼€å§‹ç¼–è¯‘ kernel çš„ä»£ç 

```python
class CPUKernel:
    def __init__(self, kernel_file="", source_code=default_cpu_kernel_code_path, compile_parallel_kernel=None, parallel_num=None):
        self.load =False
        self.int8WeightExtractionFloat = None
        self.int4WeightExtractionFloat = None
        self.int4WeightCompression = None
        self.SetNumThreads = lambda x: x
# 124
        if compile_parallel_kernel is None:
            compile_parallel_kernel = bool(int(os.cpu_count()) >= 4)

        if compile_parallel_kernel and source_code == default_cpu_kernel_code_path:
            source_code = default_cpu_parallel_kernel_code_path

        kernels = None
        
		if (not kernel_file) or (not os.path.exists(kernel_file)):
            print("No compiled kernel found.")
        	try:
                	# å¼€å§‹ç¼–è¯‘ kernel
```
