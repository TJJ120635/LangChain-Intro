## 1. å‡†å¤‡å·¥ä½œ

å®‰è£… CUDA & CuDNN

å®‰è£…æ–‡ä»¶

https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#removing-cuda-toolkit-and-driver

https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local

å®‰è£…æŒ‡å¼•

https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify

https://developer.nvidia.cn/rdp/cudnn-download

æ–°å»º conda ç¯å¢ƒ

```shell
conda create -n chat python=3.8
```

å®‰è£… pytorch

```shell
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
```

å…‹éš†é¡¹ç›®

```shell
git clone https://github.com/THUDM/ChatGLM-6B.git
cd ChatGLM-6B
```

å®‰è£…ä¾èµ–

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt
```

## 2. å¯¼å…¥æ¨¡å‹

ï¼ˆåœ¨ ChatGLM-6B ç›®å½•é‡Œï¼‰

å…‹éš† hugging face çš„ chatglm-6b

```shell
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b
```

ç„¶ååœ¨ https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b&mode=list

ä¸‹è½½ FP16 çš„æ¨¡å‹å‚æ•°ï¼Œä¸¢è¿› chatglm-6b æ–‡ä»¶å¤¹



å…‹éš† hugging face çš„ chatglm-6b-int4

```shell
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b-int4
```

ç„¶ååœ¨ https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2Fchatglm-6b-int4&mode=list

ä¸‹è½½ INT4 çš„æ¨¡å‹å‚æ•°ï¼Œä¸¢è¿› chatglm-6b-int4 æ–‡ä»¶å¤¹



## 3. è¿è¡Œ test.py(CPU)

æŒ‰ç…§ GitHub å®˜æ–¹æ–‡æ¡£â€œä»£ç è°ƒç”¨â€ç« èŠ‚è¿›è¡Œæµ‹è¯•

åªæ˜¯ä½¿ç”¨äº† CPU æ¨ç†ï¼Œé€Ÿåº¦éå¸¸æ…¢

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("chatglm-6b-int4",trust_remote_code=True).float()
model = model.eval()

print("Begin...")
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", history=history)
print(response)
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\2.png)



## 4. ä¿®å¤ CUDA åº“ï¼ˆå¯ä»¥è·³è¿‡ï¼‰

### 4.1 é—®é¢˜å‡ºç°

ç¡®è®¤ cuda å¯ç”¨

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\3.png)

ä¿®æ”¹ test.pyï¼Œä¿®æ”¹æ¨¡å‹éƒ¨åˆ†å¹¶è¿è¡Œ

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("chatglm-6b-int4",trust_remote_code=True).half().cuda()
model = model.eval()

print("First:")
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
print("Second:")
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", history=history)
print(response)
```

æ¨¡å‹è¿è¡Œæ—¶å‡ºç°æŠ¥é”™ï¼š

```shell
File "/root/anaconda3/envs/chat/lib/python3.8/site-packages/cpm_kernels/kernels/base.py", line 24, in get_module
    self._module[curr_device] = cuda.cuModuleLoadData(self._code)
  File "/root/anaconda3/envs/chat/lib/python3.8/site-packages/cpm_kernels/library/base.py", line 72, in wrapper
    raise RuntimeError("Library %s is not initialized" % self.__name)
RuntimeError: Library cuda is not initialized
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\4.png)

å®Œæ•´æŠ¥é”™ï¼š

```shell
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
No compiled kernel found.
Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c
Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so
Setting CPU quantization kernel threads to 2
Parallel kernel is not recommended when parallel num < 4.
Using quantization cache
Applying quantization to glm layers
First:
The dtype of attention mask (torch.int64) is not bool
Traceback (most recent call last):
  File "/data1/chat_model/test.py", line 14, in <module>
    response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1286, in chat
    outputs = self.generate(**inputs, **gen_kwargs)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/transformers/generation/utils.py", line 1452, in generate
    return self.sample(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/transformers/generation/utils.py", line 2468, in sample
    outputs = self(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1191, in forward
    transformer_outputs = self.transformer(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 997, in forward
    layer_ret = layer(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 445, in forward
    mixed_raw_layer = self.query_key_value(hidden_states)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 375, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 53, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 274, in extract_weight_to_half
    func(
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 48, in __call__
    func = self._prepare_func()
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 40, in _prepare_func
    self._module.get_module(), self._func_name
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/kernels/base.py", line 24, in get_module
    self._module[curr_device] = cuda.cuModuleLoadData(self._code)
  File "/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/library/base.py", line 72, in wrapper
    raise RuntimeError("Library %s is not initialized" % self.__name)
RuntimeError: Library cuda is not initialized
```

### 4.2 é—®é¢˜å®šä½

/root/anaconda3/envs/chat/lib/python3.9/site-packages/cpm_kernels/library/base.py

```python
def unix_find_lib(name):
    cuda_path = os.environ.get("CUDA_PATH", None)
    if cuda_path is not None:
        lib_name = os.path.join(cuda_path, "lib64", "lib%s.so" % name)
        if os.path.exists(lib_name):
            return lib_name

    cuda_path = "/usr/local/cuda"
    if cuda_path is not None:
        lib_name = os.path.join(cuda_path, "lib64", "lib%s.so" % name)
        if os.path.exists(lib_name):
            return lib_name

class Lib:
    def __init__(self, name):
        self.__name = name
        if sys.platform.startswith("win"):
            lib_path = windows_find_lib(self.__name)
            self.__lib_path = lib_path
            if lib_path is not None:
                self.__lib = ctypes.WinDLL(lib_path)
            else:
                self.__lib = None
        elif sys.platform.startswith("linux"):
            lib_path = unix_find_lib(self.__name)
            # Edit Here
            print(name, ':', lib_path)
            
            self.__lib_path = lib_path
            if lib_path is not None:
                self.__lib = ctypes.cdll.LoadLibrary(lib_path)
            else:
                self.__lib = None
        else:
            raise RuntimeError("Unknown platform: %s" % sys.platform)
```

![](D:\æ–‡æ¡£\ç¬”è®°\GPT\GLM\6.png)

åœ¨æ¨¡å‹åŠ è½½å‰é¦–å…ˆä¼šæœç´¢ /usr/local/cuda/lib64ï¼Œå¯¼å…¥ .so åº“

lib64 ä¸­ç¼ºå°‘äº† libcuda.soï¼Œå› æ­¤æŠ¥é”™

### 4.3 é—®é¢˜è§£å†³æ–¹æ¡ˆ

é¦–å…ˆæœç´¢ç°æœ‰çš„ libcuda.so

ä½¿ç”¨ nvidia-smi æŸ¥çœ‹é©±åŠ¨ç‰ˆæœ¬æ˜¯ 515.65.01

å‘ç°åœ¨ /usr/lib/x86_64-linux-gnu ä¸­æœ‰ libcuda.so.515.65.01

```shell
(base) root@ubuntu-virtual-machine:~# find / -name "libcuda.*"
/usr/local/cuda-11.7/targets/x86_64-linux/lib/stubs/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so.1
/usr/lib/x86_64-linux-gnu/libcuda.so.525.105.17
/usr/lib/x86_64-linux-gnu/libcuda.so.515.65.01
/usr/lib/i386-linux-gnu/libcuda.so
/usr/lib/i386-linux-gnu/libcuda.so.1
/usr/lib/i386-linux-gnu/libcuda.so.515.65.01


(base) root@ubuntu-virtual-machine:~# ll /usr/lib/x86_64-linux-gnu/libcuda*
lrwxrwxrwx 1 root root       29 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcudadebugger.so.1 -> libcudadebugger.so.525.105.17
-rw-r--r-- 1 root root 10490248 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcudadebugger.so.525.105.17
lrwxrwxrwx 1 root root       12 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1
lrwxrwxrwx 1 root root       21 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.525.105.17
-rwxr-xr-x 1 root root 20988000 4æœˆ  27 10:31 /usr/lib/x86_64-linux-gnu/libcuda.so.515.65.01*
-rw-r--r-- 1 root root 29867944 3æœˆ  27 19:40 /usr/lib/x86_64-linux-gnu/libcuda.so.525.105.17
```

å°† libcuda.so.515.65.01 å¤åˆ¶åˆ° lib64ï¼ŒæŠ¥é”™

é©±åŠ¨å’Œ library ä¸ä¸€è‡´

```shell
(chat) root@ubuntu-virtual-machine:/data1/chat_model# nvidia smi
Failed to initialize NVML: Driver/library version mismatch
```

https://www.nvidia.com/Download/Find.aspx?lang=en-us

https://www.nvidia.com/download/driverResults.aspx/191320/en-us/

æ ¹æ®è¿™ä¸ªæ‰¾åˆ°åº“å¯¹åº”çš„ 515.65.01 ç‰ˆæœ¬

ä¸‹è½½ sh NVIDIA-Linux-x86_64-515.65.01.run é‡æ–°å®‰è£…å³å¯

```shell
(base) root@ubuntu-virtual-machine:/usr/lib/x86_64-linux-gnu# ll libcuda*
lrwxrwxrwx 1 root root       29 3æœˆ  27 19:40 libcudadebugger.so.1 -> libcudadebugger.so.525.105.17
-rw-r--r-- 1 root root 10490248 3æœˆ  27 19:40 libcudadebugger.so.525.105.17
lrwxrwxrwx 1 root root       12 4æœˆ  28 09:03 libcuda.so -> libcuda.so.1*
lrwxrwxrwx 1 root root       20 4æœˆ  28 09:03 libcuda.so.1 -> libcuda.so.515.65.01*
-rwxr-xr-x 1 root root 20988000 4æœˆ  28 09:03 libcuda.so.515.65.01*
```



## 5. è¿è¡Œ test.py(GPU)

ä¿®æ”¹ test.py å¦‚ä¸‹

```python
print('----------------------------------------------')

from transformers import AutoTokenizer, AutoModel
import time

print('Tokenizer:')
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
print('Model:')
start_time = time.time()
model = AutoModel.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
print('Time:',time.time()-start_time)
print('Half:')
start_time = time.time()
model = model.half()
print('Time:',time.time()-start_time)
print('Cuda:')
start_time = time.time()
model = model.cuda()
print('Time:',time.time()-start_time)
print('Cuda loaded.')

qlist = ['ä½ å¥½ï¼', 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚', 'ä½ æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ', 'è¯·ä»æ•´ä½“ä»‹ç»ä¸€ä¸‹ç¯ä¿è¡Œ
ä¸šã€‚', 'è¯·å†™ä¸€æ®µä½¿ç”¨pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ã€‚']

history = []
for query in qlist:
    print(query)
    start_time = time.time()
    response, history = model.chat(tokenizer, query, history=history)
    print(response)
    print('Time:',time.time()-start_time)
```

Modelï¼š10-15s

Halfï¼š0.36s

Cudaï¼š4-15s

````shell
(chat) root@ubuntu-virtual-machine:/data1/chat_model# python test.py
----------------------------------------------
Tokenizer:
Model:
Begin model=cls()
nvrtc : /usr/local/cuda/lib64/libnvrtc.so
cuda : /usr/local/cuda/lib64/libcuda.so
cudart : /usr/local/cuda/lib64/libcudart.so
cublasLt : /usr/local/cuda/lib64/libcublasLt.so
Cannot load cpu kernel, don't use quantized model on cpu.
Using quantization cache
Applying quantization to glm layers
End model=cls()
Load from pt
Loaded from pt
Model evaled
Time: 10.415313243865967
Half:
Time: 0.36229372024536133
Cuda:
Time: 11.275478839874268
Cuda loaded.
ä½ å¥½ï¼
The dtype of attention mask (torch.int64) is not bool
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
Time: 11.327528953552246
è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚
æˆ‘å¯ä»¥å›ç­”å…³äºæˆ‘è‡ªå·±çš„ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚æˆ‘çš„è®­ç»ƒæ•°æ®é›†ï¼Œæˆ‘çš„ç®—æ³•æ¨¡å‹å’Œè¿è¡Œæ–¹å¼ç­‰ç­‰ã€‚åŒæ—¶ï¼Œæˆ‘ä¹Ÿå¯ä»¥æä¾›ä¸€äº›å…³äºæˆ‘çš„ä¸»äºº ChatGLM-6B çš„ä¿¡æ¯ï¼Œå¦‚æœå¯¹å¥¹æ„Ÿå…´è¶£çš„è¯ã€‚
Time: 9.367414236068726
ä½ æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ
ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘å…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæˆ‘å¯ä»¥ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œè¯­éŸ³ã€‚

2. çŸ¥è¯†åº“ï¼šæˆ‘å¯ä»¥è®¿é—®å’Œå­¦ä¹ å¤§é‡çš„æ–‡æœ¬å’Œæ•°æ®é›†ï¼Œä»¥ä¾¿å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

3. è¯­è¨€ç”Ÿæˆï¼šæˆ‘å¯ä»¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œä¾‹å¦‚æ–‡ç« ã€å›å¤å’Œå¯¹è¯ã€‚

4. è‡ªåŠ¨æ‘˜è¦ï¼šæˆ‘å¯ä»¥è‡ªåŠ¨æå–æ–‡æœ¬ä¸­çš„é‡ç‚¹å’Œæ‘˜è¦ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥å¿«é€Ÿäº†è§£æ–‡æœ¬å†…å®¹ã€‚

5. å¯¹è¯ï¼šæˆ‘å¯ä»¥ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œå›ç­”ä»–ä»¬çš„é—®é¢˜æˆ–æ‰§è¡Œå…¶ä»–ä»»åŠ¡ï¼Œä¾‹å¦‚æä¾›å»ºè®®æˆ–æ‰§è¡Œè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚

å¸Œæœ›è¿™äº›æ¦‚æ‹¬å¯ä»¥æä¾›å¸®åŠ©ï¼
Time: 28.96828293800354
è¯·ä»æ•´ä½“ä»‹ç»ä¸€ä¸‹ç¯ä¿è¡Œä¸šã€‚
ç¯ä¿è¡Œä¸šæ˜¯æŒ‡ä¸“é—¨ä»äº‹ç¯å¢ƒä¿æŠ¤å’Œæ¸…æ´å‘å±•çš„è¡Œä¸šï¼Œæ—¨åœ¨é€šè¿‡è§£å†³ç¯å¢ƒé—®é¢˜æ¥æ”¹å–„ç¯å¢ƒè´¨é‡ï¼Œæé«˜å¯æŒç»­å‘å±•æ°´å¹³ã€‚ç¯ä¿è¡Œä¸šæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ç¯ä¿æŠ€æœ¯ã€ç¯å¢ƒç›‘æµ‹ã€ç¯å¢ƒæ²»ç†ã€ç¯ä¿ææ–™ã€æ¸…æ´æœåŠ¡ã€ç¯ä¿æ³•è§„ç­‰ã€‚

åœ¨ç¯ä¿è¡Œä¸šä¸­ï¼Œç¯ä¿æŠ€æœ¯æ˜¯æŒ‡åˆ©ç”¨å„ç§æŠ€æœ¯å’Œæ–¹æ³•æ¥å‡å°‘ç¯å¢ƒæ±¡æŸ“å’Œå¤„ç†æ±¡æŸ“ç‰©çš„æ–¹æ³•ã€‚ç¯å¢ƒç›‘æµ‹æ˜¯æŒ‡å¯¹ç¯å¢ƒæ±¡æŸ“å’Œæ°´è´¨è¿›è¡Œæ£€æµ‹å’Œåˆ†æï¼Œä»¥ä¾¿è¯†åˆ«æ±¡æŸ“ç‰©å¹¶é‡‡å–ç›¸åº”çš„æªæ–½ã€‚ç¯å¢ƒæ²»ç†æ˜¯æŒ‡å¯¹ç¯å¢ƒæ±¡æŸ“è¿›è¡Œæ²»ç†ï¼ŒåŒ…æ‹¬å¯¹æ±¡æŸ“ç‰©è¿›è¡Œå›æ”¶ã€å¤„ç†å’Œè½¬åŒ–ã€‚

ç¯ä¿ææ–™æ˜¯æŒ‡ç”¨äºç¯ä¿é¢†åŸŸçš„ææ–™ï¼Œè¿™äº›ææ–™å¯ä»¥ç”¨äºåˆ¶é€ ç¯ä¿äº§å“ï¼Œå‡å°‘å¯¹ç¯å¢ƒçš„æ±¡æŸ“ã€‚æ¸…æ´æœåŠ¡æ˜¯æŒ‡æä¾›æ¸…æ´æœåŠ¡çš„å…¬å¸ï¼ŒåŒ…æ‹¬å®¶åº­æ¸…æ´ã€å•†ä¸šæ¸…æ´å’Œå·¥ä¸šæ¸…æ´ç­‰ã€‚

ç¯ä¿æ³•è§„æ˜¯æŒ‡æœ‰å…³ç¯å¢ƒä¿æŠ¤çš„æ³•å¾‹å’Œæ”¿ç­–ï¼Œæ—¨åœ¨ä¿æŠ¤ç¯å¢ƒå’Œä¿ƒè¿›å¯æŒç»­å‘å±•ã€‚è¿™äº›æ³•è§„æ—¨åœ¨å‡å°‘ç¯å¢ƒæ±¡æŸ“ã€ä¿æŠ¤ç”Ÿæ€å¹³è¡¡ã€åŠ å¼ºç¯å¢ƒç›‘ç®¡å’Œæ¨åŠ¨ç¯ä¿æŠ€æœ¯çš„åº”ç”¨ã€‚

ç¯ä¿è¡Œä¸šæ˜¯ä¸€ä¸ªæ¶‰åŠä¼—å¤šé¢†åŸŸï¼Œæ—¨åœ¨ä¿æŠ¤ç¯å¢ƒå’Œæé«˜å¯æŒç»­å‘å±•æ°´å¹³çš„è¡Œä¸šã€‚åœ¨æ—¥ç›Šé‡è§†ç¯å¢ƒä¿æŠ¤çš„å½“ä»Šä¸–ç•Œï¼Œç¯ä¿è¡Œä¸šçš„å‰æ™¯çœ‹å¥½ï¼ŒåŒæ—¶ä¹Ÿé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚
Time: 46.70101070404053
è¯·å†™ä¸€æ®µä½¿ç”¨pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ã€‚
ä»¥ä¸‹æ˜¯ä½¿ç”¨Pythonè¿›è¡ŒPCAé™ç»´å¯è§†åŒ–çš„ä»£ç ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰æ•°æ®
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# ä½¿ç”¨PCAè¿›è¡Œé™ç»´
pca = np.random.randn(data.shape[0], data.shape[1]+1)
pca[:,1] = data

# æ‰“å°PCAç»“æœ
print("PCAç»“æœï¼š")
print(pca.shape)

# å¯è§†åŒ–PCAç»“æœ
data_pca = pca.reshape((data.shape[0], data.shape[1]+1))
plt.imshow(data_pca, cmap='gray', aspect='auto', extent=[0, 10, 0, 10])
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Pca é™ç»´ç»“æœ')
plt.show()
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œå°†æ•°æ®ç»è¿‡PCAé™ç»´åå­˜å‚¨åœ¨å˜é‡ `data_pca` ä¸­ï¼Œç„¶åä½¿ç”¨ `imshow` å‡½æ•°å°†é™ç»´åçš„æ•°æ®å’ŒPCAç»“æœå¯è§†åŒ–ã€‚
Time: 60.72647047042847
````

## 6. è¿è¡Œæƒ…å†µ

| æ¨¡å‹ | å†…å­˜(G)   | æ˜¾å­˜(G) | åŠ è½½(s) | çŸ­å¥(s) | é•¿æ®µ(s) |
| ---- | --------- | ------- | ------- | ------- | ------- |
| int4 | 6.5/8.5   | 5       | 15-20   | 3-10    | 30-60   |
| int8 | 14.5      | 7-9     | 100-120 | 10-20   | 30-100  |
| 6b   | 13.6/14.5 | 12-13   | 140-250 | 3-10    | 10-40   |

## 7. æŠ¥é”™æç¤ºé—®é¢˜

### 6.1 revision æŠ¥é”™

è¿™ä¸ªæ˜¯æ²¡æœ‰æä¾›æ¨¡å‹ç‰ˆæœ¬å·çš„é—®é¢˜

åŠ å…¥ `revision=` å‚æ•°è§£å†³ï¼Œç‰ˆæœ¬å·åç§°å¯ä»¥è‡ªå®š

```python
tokenizer = AutoTokenizer.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
model = AutoModel.from_pretrained("chatglm-6b-int4", trust_remote_code=True, revision='int4')
```

### 6.2 No compiled kernel found

åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µï¼Œé¦–å…ˆä¼šåŠ è½½ä¸€ä¸ªé‡åŒ–ç”¨çš„ kernel

æ¨¡å‹åŠ è½½åˆ°å†…å­˜åä½¿ç”¨ kernel è¿›è¡Œé‡åŒ–ï¼Œå†åŠ è½½åˆ° GPU ä¸­



chatglm-6b-int4/quantization.py çš„é‡åŒ–éƒ¨åˆ†

kwargs ä¸ºç©ºï¼Œload_cpu_kernel æ²¡æœ‰å‚æ•°ä¼ è¿›æ¥

```python
# 441
def quantize(model, weight_bit_width, use_quantization_cache=False, empty_init=False, **kwargs):
    """Replace fp16 linear with quantized linear"""

    query_key_value_quantization_cache = None
    dense_quantization_cache = None
    dense_h_to_4h_quantization_cache = None
    dense_4h_to_h_quantization_cache = None

    try:
        # ä¿®æ”¹è¿™ä¸€å¥
        # load_cpu_kernel(**kwargs)
        load_cpu_kernel(kernel_file='quantization_kernels_parallel.so', **kwargs)
    except:
        if kernels is None:  # CUDA kernels failed
            print("Cannot load cpu or cuda kernel, quantization failed:")
            assert kernels is not None
        print("Cannot load cpu kernel, don't use quantized model on cpu.")
```

å¯¼è‡´ kernel_file ä¸ºç©º

```python
class CPUKernel:
    def __init__(self, kernel_file="", source_code=default_cpu_kernel_code_path, compile_parallel_kernel=None, parallel_num=None):
        self.load =False
        self.int8WeightExtractionFloat = None
        self.int4WeightExtractionFloat = None
        self.int4WeightCompression = None
        self.SetNumThreads = lambda x: x
# 124
        if compile_parallel_kernel is None:
            compile_parallel_kernel = bool(int(os.cpu_count()) >= 4)

        if compile_parallel_kernel and source_code == default_cpu_kernel_code_path:
            source_code = default_cpu_parallel_kernel_code_path

        kernels = None
        
		if (not kernel_file) or (not os.path.exists(kernel_file)):
            print("No compiled kernel found.")
        	try:
                	# å¼€å§‹ç¼–è¯‘ kernel
```



å¾…ä½¿ç”¨çš„ä¾‹å­ï¼š

æˆ‘æ­£åœ¨ç¼–å†™ä¸€ä¸ª Python ç¨‹åºï¼Œä½¿ç”¨ text2vec æ¥è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ã€‚æˆ‘çš„ç¨‹åºå¦‚ä¸‹ï¼š

```
# Text Embedding Program
from text2vec import SentenceModel
model = SentenceModel(r'D:\Projects\text2vec-base-chinese')
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡', 'å¦‚ä½•åŠç†é“¶è¡Œå¡', 'å“ˆé‡Œè·¯å¤§æ—‹é£']
embeddings = model.encode(sentences)
print(embeddings)
```
æ ¹æ®æ¨¡å‹çš„ç¼–ç ï¼Œæˆ‘å¾—åˆ°äº†ä¸€ä¸ªåä¸º embeddings çš„æ•°ç»„ã€‚embeddings æ•°ç»„åŒ…å«4ä¸ªå‘é‡ï¼Œæ¯ä¸ªå‘é‡å¯¹åº”ä¸€ä¸ªå¥å­ï¼Œæ¯ä¸ªå‘é‡æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°çš„ list
embeddings çš„å½¢å¼ç±»ä¼¼äºï¼š
print(embeddings)

[[-4.4390789e-04 -2.9734781e-01  8.5790151e-01 ... -5.2770150e-01
  -1.4315721e-01 -1.0007849e-01]
 [ 6.5362066e-01 -7.6667674e-02  9.5962387e-01 ... -6.0122484e-01
  -1.6792282e-03  2.1457718e-01]
 [ 3.3719593e-01 -7.9632360e-01  3.8019526e-01 ... -1.9252342e-01
   7.5160302e-02 -1.6093762e-01]
 [-8.8900603e-02 -2.4561442e-02  4.1167963e-01 ...  1.2363576e+00
  -3.4643823e-01 -7.1788603e-01]]
å…¶ä¸­ ... ä»£è¡¨çœç•¥äº†ä¸­é—´çš„ç»´åº¦

ç°åœ¨ï¼Œæˆ‘æƒ³è¦ä½¿ç”¨å¯è§†åŒ–çš„æ–¹æ³•å±•ç¤º embeddings ä¸­4ä¸ªå‘é‡çš„å·®åˆ«ï¼Œè¿™åŒæ—¶ä¹Ÿæ„å‘³ç€4ä¸ªå¥å­çš„å·®åˆ«ã€‚è¯·å¸®æˆ‘ç¼–å†™ä¸€ä¸ªpythonç¨‹åºæ¥å®ç°å¯è§†åŒ– 
